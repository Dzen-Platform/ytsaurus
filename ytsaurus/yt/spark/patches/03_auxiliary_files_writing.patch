commit e06d1e926f1cd797552cf0f9b72e9642599d3dd4
author: alex-shishkin
date: 2022-10-17T16:45:33+03:00

    auxiliary_files_writing

--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/AddressUtils.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/AddressUtils.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.deploy
+
+import java.io.{File, PrintWriter}
+
+import com.fasterxml.jackson.databind.ObjectMapper
+import com.fasterxml.jackson.module.scala.DefaultScalaModule
+import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper
+
+import org.apache.spark.internal.Logging
+
+private[deploy] object AddressUtils extends Logging {
+  def writeAddressToFile(name: String,
+                         host: String,
+                         port: Int,
+                         webUiPort: Option[Int],
+                         restPort: Option[Int]): Unit = {
+    logInfo(s"Writing address to file: $port, $webUiPort, $restPort")
+
+    val mapper = new ObjectMapper() with ScalaObjectMapper
+    mapper.registerModule(DefaultScalaModule)
+    val addressString = mapper.writeValueAsString(Map(
+      "host" -> host,
+      "port" -> port,
+      "webUiPort" -> webUiPort,
+      "restPort" -> restPort
+    ))
+
+    val pw = new PrintWriter(new File(s"${name}_address"))
+    try {
+      pw.write(addressString)
+      require(new File(s"${name}_address_success").createNewFile())
+    } finally {
+      pw.close()
+    }
+  }
+
+}
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
@@ -21,19 +21,21 @@ import java.util.NoSuchElementException
 import java.util.zip.ZipOutputStream
 import javax.servlet.http.{HttpServlet, HttpServletRequest, HttpServletResponse}
 
+import scala.reflect.internal.util.ScalaClassLoader
 import scala.util.control.NonFatal
 import scala.xml.Node
 
 import org.eclipse.jetty.servlet.{ServletContextHandler, ServletHolder}
 
 import org.apache.spark.{SecurityManager, SparkConf}
-import org.apache.spark.deploy.SparkHadoopUtil
+import org.apache.spark.deploy.{AddressUtils, SparkHadoopUtil}
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config._
 import org.apache.spark.internal.config.History
 import org.apache.spark.internal.config.UI._
 import org.apache.spark.status.api.v1.{ApiRootResource, ApplicationInfo, UIRoot}
 import org.apache.spark.ui.{SparkUI, UIUtils, WebUI}
+import org.apache.spark.ui.JettyUtils.createServletHandler
 import org.apache.spark.util.{ShutdownHookManager, SystemClock, Utils}
 
 /**
@@ -309,6 +311,7 @@ object HistoryServer extends Logging {
     val server = new HistoryServer(conf, provider, securityManager, port)
     server.bind()
     provider.start()
+    AddressUtils.writeAddressToFile("history", server.publicHostName, server.boundPort, None, None)
 
     ShutdownHookManager.addShutdownHook { () => server.stop() }
 
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/master/Master.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/master/Master.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
@@ -25,7 +25,7 @@ import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}
 import scala.util.Random
 
 import org.apache.spark.{SecurityManager, SparkConf, SparkException}
-import org.apache.spark.deploy.{ApplicationDescription, DriverDescription, ExecutorState, SparkHadoopUtil}
+import org.apache.spark.deploy.{AddressUtils, ApplicationDescription, DriverDescription, ExecutorState, SparkHadoopUtil}
 import org.apache.spark.deploy.DeployMessages._
 import org.apache.spark.deploy.master.DriverState.DriverState
 import org.apache.spark.deploy.master.MasterMessages._
@@ -165,6 +165,8 @@ private[deploy] class Master(
       restServer = Some(new StandaloneRestServer(address.host, port, conf, self, masterUrl))
     }
     restServerBoundPort = restServer.map(_.start())
+    AddressUtils.writeAddressToFile("master", masterPublicAddress,
+      address.port, Some(webUi.boundPort), restServerBoundPort)
 
     masterMetricsSystem.registerSource(masterSource)
     masterMetricsSystem.start()
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
@@ -17,22 +17,26 @@
 
 package org.apache.spark.deploy.rest
 
-import java.io.{DataOutputStream, FileNotFoundException}
-import java.net.{ConnectException, HttpURLConnection, SocketException, URL}
+import java.io.{DataOutputStream, File, FileNotFoundException}
+import java.net.{ConnectException, HttpURLConnection, SocketException, URI, URL}
 import java.nio.charset.StandardCharsets
 import java.util.concurrent.TimeoutException
 import javax.servlet.http.HttpServletResponse
 
+import scala.annotation.tailrec
 import scala.collection.mutable
 import scala.concurrent.{Await, Future}
 import scala.concurrent.duration._
 import scala.io.Source
+import scala.util.{Failure, Random, Success, Try}
 import scala.util.control.NonFatal
 
 import com.fasterxml.jackson.core.JsonProcessingException
+import org.apache.commons.io.FileUtils
+import org.apache.hadoop.fs.FileSystem
 
 import org.apache.spark.{SPARK_VERSION => sparkVersion, SparkConf, SparkException}
-import org.apache.spark.deploy.SparkApplication
+import org.apache.spark.deploy.{SparkApplication, SparkHadoopUtil}
 import org.apache.spark.internal.Logging
 import org.apache.spark.util.Utils
 
@@ -428,9 +432,16 @@ private[spark] object RestSubmissionClient {
   private[spark] def supportsRestClient(master: String): Boolean = {
     supportedMasterPrefixes.exists(master.startsWith)
   }
+
+  private[rest] def filterSparkConf(conf: SparkConf): Map[String, String] = {
+    conf.getAll.collect {
+      case (key, value) if key.startsWith("spark.yt") || key.startsWith("spark.hadoop.yt") =>
+        key.toUpperCase().replace(".", "_") -> value
+    }.toMap
+  }
 }
 
-private[spark] class RestSubmissionClientApp extends SparkApplication {
+private[spark] class RestSubmissionClientApp extends SparkApplication with Logging {
 
   /** Submits a request to run the application and return the response. Visible for testing. */
   def run(
@@ -439,8 +450,8 @@ private[spark] class RestSubmissionClientApp extends SparkApplication {
       appArgs: Array[String],
       conf: SparkConf,
       env: Map[String, String] = Map()): SubmitRestProtocolResponse = {
-    val master = conf.getOption("spark.master").getOrElse {
-      throw new IllegalArgumentException("'spark.master' must be set.")
+    val master = conf.getOption("spark.rest.master").getOrElse {
+      throw new IllegalArgumentException("'spark.rest.master' must be set.")
     }
     val sparkProperties = conf.getAll.toMap
     val client = new RestSubmissionClient(master)
@@ -449,15 +460,101 @@ private[spark] class RestSubmissionClientApp extends SparkApplication {
     client.createSubmission(submitRequest)
   }
 
+  @tailrec
+  private def getSubmissionStatus(submissionId: String,
+                                  client: RestSubmissionClient,
+                                  retry: Int,
+                                  retryInterval: Duration,
+                                  rnd: Random = new Random): SubmissionStatusResponse = {
+    val response = Try(client.requestSubmissionStatus(submissionId)
+      .asInstanceOf[SubmissionStatusResponse])
+    response match {
+      case Success(value) => value
+      case Failure(exception) if retry > 0 =>
+        log.error(s"Exception while getting submission status: ${exception.getMessage}")
+        val sleepInterval = if (retryInterval > 1.second) {
+          1000 + rnd.nextInt(retryInterval.toMillis.toInt - 1000)
+        } else rnd.nextInt(retryInterval.toMillis.toInt)
+        Thread.sleep(sleepInterval)
+        getSubmissionStatus(submissionId, client, retry - 1, retryInterval, rnd)
+      case Failure(exception) => throw exception
+    }
+  }
+
+  def awaitAppTermination(submissionId: String,
+                          conf: SparkConf,
+                          checkStatusInterval: Duration): Unit = {
+    import org.apache.spark.deploy.master.DriverState._
+
+    val master = conf.getOption("spark.rest.master").getOrElse {
+      throw new IllegalArgumentException("'spark.rest.master' must be set.")
+    }
+    val client = new RestSubmissionClient(master)
+    val runningStates = Set(RUNNING.toString, SUBMITTED.toString)
+    val finalStatus = Stream.continually {
+      Thread.sleep(checkStatusInterval.toMillis)
+      val response = getSubmissionStatus(submissionId, client, retry = 3, checkStatusInterval)
+      logInfo(s"Driver report for $submissionId (state: ${response.driverState})")
+      response
+    }.find(response => !runningStates.contains(response.driverState)).get
+    logInfo(s"Driver $submissionId finished with status ${finalStatus.driverState}")
+    finalStatus.driverState match {
+      case s if s == FINISHED.toString => // success
+      case s if s == FAILED.toString =>
+        throw new SparkException(s"Driver $submissionId failed")
+      case _ =>
+        throw new SparkException(s"Driver $submissionId failed with unexpected error")
+    }
+  }
+
+  def shutdownYtClient(sparkConf: SparkConf): Unit = {
+    val hadoopConf = SparkHadoopUtil.newConfiguration(sparkConf)
+    val fs = FileSystem.get(new URI("yt:///"), hadoopConf)
+    fs.close()
+  }
+
+  private def writeToFile(file: File, message: String): Unit = {
+    val tmpFile = new File(file.getParentFile, s"${file.getName}_tmp")
+    FileUtils.writeStringToFile(tmpFile, message)
+    FileUtils.moveFile(tmpFile, file)
+  }
+
   override def start(args: Array[String], conf: SparkConf): Unit = {
-    if (args.length < 2) {
-      sys.error("Usage: RestSubmissionClient [app resource] [main class] [app args*]")
-      sys.exit(1)
+    val submissionIdFile = conf.getOption("spark.rest.client.submissionIdFile").map(new File(_))
+    val submissionErrorFile = conf.getOption("spark.rest.client.submissionErrorFile")
+      .map(new File(_))
+    try {
+      if (args.length < 2) {
+        sys.error("Usage: RestSubmissionClient [app resource] [main class] [app args*]")
+        sys.exit(1)
+      }
+      val appResource = args(0)
+      val mainClass = args(1)
+      val appArgs = args.slice(2, args.length)
+      val env = RestSubmissionClient.filterSystemEnvironment(sys.env) ++
+        RestSubmissionClient.filterSparkConf(conf)
+
+      val submissionId = try {
+        val response = run(appResource, mainClass, appArgs, conf, env)
+        response match {
+          case r: CreateSubmissionResponse => r.submissionId
+          case _ => throw new IllegalStateException("Job is not submitted")
+        }
+      } finally {
+        shutdownYtClient(conf)
+      }
+
+      submissionIdFile.foreach(writeToFile(_, submissionId))
+
+      if (conf.getOption("spark.rest.client.awaitTermination.enabled").forall(_.toBoolean)) {
+        val checkStatusInterval = conf.getOption("spark.rest.client.statusInterval")
+          .map(_.toInt.seconds).getOrElse(5.seconds)
+        awaitAppTermination(submissionId, conf, checkStatusInterval)
+      }
+    } catch {
+      case e: Throwable =>
+        submissionErrorFile.foreach(writeToFile(_, e.getMessage))
+        throw e
     }
-    val appResource = args(0)
-    val mainClass = args(1)
-    val appArgs = args.slice(2, args.length)
-    val env = RestSubmissionClient.filterSystemEnvironment(sys.env)
-    run(appResource, mainClass, appArgs, conf, env)
   }
 }
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
@@ -30,7 +30,7 @@ import scala.util.{Failure, Random, Success}
 import scala.util.control.NonFatal
 
 import org.apache.spark.{SecurityManager, SparkConf}
-import org.apache.spark.deploy.{Command, ExecutorDescription, ExecutorState}
+import org.apache.spark.deploy.{AddressUtils, Command, ExecutorDescription, ExecutorState}
 import org.apache.spark.deploy.DeployMessages._
 import org.apache.spark.deploy.ExternalShuffleService
 import org.apache.spark.deploy.StandaloneResourceUtils._
@@ -242,6 +242,8 @@ private[deploy] class Worker(
     metricsSystem.start()
     // Attach the worker metrics servlet handler to the web ui after the metrics system is started.
     metricsSystem.getServletHandlers.foreach(webUi.attachHandler)
+
+    AddressUtils.writeAddressToFile("worker", host, webUi.boundPort, None, None)
   }
 
   private def setupWorkerResources(): Unit = {
