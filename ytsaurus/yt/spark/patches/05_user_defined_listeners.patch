commit 3769b9290f43a039a52a755aaff9801b6418168d
author: alex-shishkin
date: 2022-10-17T16:48:30+03:00

    user_defined_listeners

--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala	(3769b9290f43a039a52a755aaff9801b6418168d)
@@ -578,3 +578,7 @@ abstract class SparkListener extends SparkListenerInterface {
 
   override def onResourceProfileAdded(event: SparkListenerResourceProfileAdded): Unit = { }
 }
+
+abstract class UserDefinedSparkListener extends SparkListener {
+  def onListenerStart(): Unit = {}
+}
--- taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/StaticSQLConf.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
+++ taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/StaticSQLConf.scala	(3769b9290f43a039a52a755aaff9801b6418168d)
@@ -127,6 +127,14 @@ object StaticSQLConf {
     .toSequence
     .createOptional
 
+  val SPARK_SESSION_LISTENERS = buildStaticConf("spark.context.listeners")
+    .doc("A comma-separated list of classes that implement " +
+      "UserDefinedSparkListener added to Spark Session listeners.")
+    .version("2.2.0")
+    .stringConf
+    .toSequence
+    .createOptional
+
   val SPARK_CACHE_SERIALIZER = buildStaticConf("spark.sql.cache.serializer")
     .doc("The name of a class that implements " +
       "org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to " +
--- taxi/dmp/spark/spark/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
+++ taxi/dmp/spark/spark/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala	(3769b9290f43a039a52a755aaff9801b6418168d)
@@ -32,7 +32,7 @@ import org.apache.spark.api.java.JavaRDD
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config.{ConfigEntry, EXECUTOR_ALLOW_SPARK_CONTEXT}
 import org.apache.spark.rdd.RDD
-import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}
+import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd, UserDefinedSparkListener}
 import org.apache.spark.sql.catalog.Catalog
 import org.apache.spark.sql.catalyst._
 import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation
@@ -961,6 +961,8 @@ object SparkSession extends Logging {
         registerContextListener(sparkContext)
       }
 
+      addUserDefinedListeners(session)
+
       return session
     }
 
@@ -1223,4 +1225,30 @@ object SparkSession extends Logging {
       }
     }
   }
+
+  /**
+   * Initialize listeners for given classnames. The classes will be added to the
+   * SparkContext passed into this function.
+   */
+    def addUserDefinedListeners(spark: SparkSession): Unit = {
+    logInfo("Add user defined spark listeners")
+    val udlConfClassNames = spark.sparkContext.getConf
+      .get(StaticSQLConf.SPARK_SESSION_LISTENERS).getOrElse(Seq.empty)
+    udlConfClassNames.foreach { udlConfClassName =>
+      try {
+        val udlConfClass = Utils.classForName(udlConfClassName)
+        val udlc = udlConfClass.getConstructor(classOf[SparkSession])
+        val udli: UserDefinedSparkListener = udlc.newInstance(spark)
+        udli.onListenerStart()
+        spark.sparkContext.addSparkListener(udli)
+        logInfo(s"Added user defined spark listener $udlConfClassName")
+      } catch {
+        // Ignore the error if we cannot find the class or when the class has the wrong type.
+        case e@(_: ClassCastException |
+                _: ClassNotFoundException |
+                _: NoClassDefFoundError) =>
+          logWarning(s"Cannot use $udlConfClassName to configure session listeners.", e)
+      }
+    }
+  }
 }
