commit 5f95d55d0aeec435d71fc82fdb11e19f3df41fe8
author: alex-shishkin
date: 2022-10-17T16:36:42+03:00

    executable_features

--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/SparkEnv.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/SparkEnv.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -30,7 +30,7 @@ import com.google.common.cache.CacheBuilder
 import org.apache.hadoop.conf.Configuration
 
 import org.apache.spark.annotation.DeveloperApi
-import org.apache.spark.api.python.PythonWorkerFactory
+import org.apache.spark.api.python.{ExecutableUtils, ExecutableWorkerFactory, PythonWorkerFactory}
 import org.apache.spark.broadcast.BroadcastManager
 import org.apache.spark.internal.{config, Logging}
 import org.apache.spark.internal.config._
@@ -112,13 +112,33 @@ class SparkEnv (
     }
   }
 
+  private val isRunningExecutable = conf.getBoolean("spark.yt.isExecutable", false)
+
+  private def workerKey(
+      pythonExec: String,
+      envVars: Map[String, String]): (String, Map[String, String]) = {
+    if (isRunningExecutable) {
+      (ExecutableUtils.executablePath, envVars)
+    } else {
+      (pythonExec, envVars)
+    }
+  }
+
   private[spark]
   def createPythonWorker(
       pythonExec: String,
       envVars: Map[String, String]): (java.net.Socket, Option[Int]) = {
     synchronized {
-      val key = (pythonExec, envVars)
-      pythonWorkers.getOrElseUpdate(key, new PythonWorkerFactory(pythonExec, envVars)).create()
+      val key = workerKey(pythonExec, envVars)
+      val (path, env) = key
+      val workerFactory = pythonWorkers.getOrElseUpdate(key, {
+        if (isRunningExecutable) {
+          new ExecutableWorkerFactory(path, env)
+        } else {
+          new PythonWorkerFactory(path, env)
+        }
+      })
+      pythonWorkers.getOrElseUpdate(key, workerFactory).create()
     }
   }
 
@@ -126,7 +146,7 @@ class SparkEnv (
   def destroyPythonWorker(pythonExec: String,
       envVars: Map[String, String], worker: Socket): Unit = {
     synchronized {
-      val key = (pythonExec, envVars)
+      val key = workerKey(pythonExec, envVars)
       pythonWorkers.get(key).foreach(_.stopWorker(worker))
     }
   }
@@ -135,7 +155,7 @@ class SparkEnv (
   def releasePythonWorker(pythonExec: String,
       envVars: Map[String, String], worker: Socket): Unit = {
     synchronized {
-      val key = (pythonExec, envVars)
+      val key = workerKey(pythonExec, envVars)
       pythonWorkers.get(key).foreach(_.releaseWorker(worker))
     }
   }
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/api/python/ExecutableUtils.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/api/python/ExecutableUtils.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.api.python
+
+import java.io.File
+
+import org.apache.hadoop.fs.Path
+
+import org.apache.spark.{SparkEnv, SparkFiles}
+
+private[spark] object ExecutableUtils {
+
+  // executable must know about python files provided by spark.yt.pyFiles before the process starts,
+  // otherwise, if there are top-level imports from those python files, they will fail
+  def pythonPath: String = {
+    val ytPyFiles = SparkEnv.get.conf.get("spark.yt.pyFiles", "")
+    val localPyFiles = ytPyFiles.split(",").map(new Path(_).getName)
+    localPyFiles.map(SparkFiles.get).mkString(File.pathSeparator)
+  }
+
+  def executablePath: String = {
+    val execName = new Path(SparkEnv.get.conf.get("spark.yt.executableResource")).getName
+    SparkFiles.get(execName)
+  }
+}
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/api/python/ExecutableWorkerFactory.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/api/python/ExecutableWorkerFactory.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -0,0 +1,169 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.api.python
+
+import java.io._
+import java.net.{InetAddress, ServerSocket, Socket}
+
+import scala.collection.JavaConverters._
+
+import org.apache.spark._
+import org.apache.spark.api.python.PythonWorkerFactory.PROCESS_WAIT_TIMEOUT_MS
+import org.apache.spark.deploy.ExecutableRunMode
+import org.apache.spark.util.Utils
+
+// Copied from PythonWorkerFactory and adjusted to work with executable files
+private[spark] class ExecutableWorkerFactory(execPath: String, envVars: Map[String, String])
+  extends PythonWorkerFactory(execPath, envVars) {
+
+  override val pythonPath = PythonUtils.mergePythonPaths(
+    PythonUtils.sparkPythonPath,
+    ExecutableUtils.pythonPath,
+    envVars.getOrElse("PYTHONPATH", ""),
+    sys.env.getOrElse("PYTHONPATH", ""))
+
+  override protected def createSimpleWorker(): (Socket, Option[Int]) = {
+    var serverSocket: ServerSocket = null
+    try {
+      serverSocket = new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))
+
+      // Create and start the worker
+      val pb = new ProcessBuilder(Seq(execPath).asJava)
+      val workerEnv = pb.environment()
+      workerEnv.putAll(envVars.asJava)
+      workerEnv.put("PYTHONPATH", pythonPath)
+      // This is equivalent to setting the -u flag; we use it because ipython doesn't support -u:
+      workerEnv.put("PYTHONUNBUFFERED", "YES")
+      workerEnv.put("PYTHON_WORKER_FACTORY_PORT", serverSocket.getLocalPort.toString)
+      workerEnv.put("PYTHON_WORKER_FACTORY_SECRET", authHelper.secret)
+      workerEnv.put(ExecutableRunMode.ENV_VARIABLE_NAME, ExecutableRunMode.WORKER)
+
+      val worker = pb.start()
+
+      // Redirect worker stdout and stderr
+      redirectStreamsToStderr(worker.getInputStream, worker.getErrorStream)
+
+      // Wait for it to connect to our socket, and validate the auth secret.
+      serverSocket.setSoTimeout(10000)
+
+      try {
+        val socket = serverSocket.accept()
+        authHelper.authClient(socket)
+        val pid = new DataInputStream(socket.getInputStream).readInt()
+        if (pid < 0) {
+          throw new IllegalStateException("Python failed to launch worker with code " + pid)
+        }
+        synchronized {
+          simpleWorkers.put(socket, worker)
+        }
+        return (socket, Some(pid))
+      } catch {
+        case e: Exception =>
+          throw new SparkException("Executable worker failed to connect back.", e)
+      }
+    } finally {
+      if (serverSocket != null) {
+        serverSocket.close()
+      }
+    }
+    null
+  }
+
+  override protected def startDaemon() {
+    synchronized {
+      // Is it already running?
+      if (daemon != null) {
+        return
+      }
+
+      try {
+        // Create and start the daemon
+        val command = Seq(execPath).asJava
+        val pb = new ProcessBuilder(command)
+        val workerEnv = pb.environment()
+        workerEnv.putAll(envVars.asJava)
+        workerEnv.put("PYTHONPATH", pythonPath)
+        workerEnv.put("PYTHON_WORKER_FACTORY_SECRET", authHelper.secret)
+        // This is equivalent to setting the -u flag; we use it because ipython doesn't support -u:
+        workerEnv.put("PYTHONUNBUFFERED", "YES")
+        workerEnv.put(ExecutableRunMode.ENV_VARIABLE_NAME, ExecutableRunMode.DAEMON)
+
+        daemon = pb.start()
+
+        val in = new DataInputStream(daemon.getInputStream)
+        try {
+          daemonPort = in.readInt()
+        } catch {
+          case _: EOFException =>
+            throw new SparkException(s"No port number in $daemonModule's stdout")
+        }
+
+        // test that the returned port number is within a valid range.
+        // note: this does not cover the case where the port number
+        // is arbitrary data but is also coincidentally within range
+        if (daemonPort < 1 || daemonPort > 0xffff) {
+          val exceptionMessage = f"""
+            |Bad data in $daemonModule's standard output. Invalid port number:
+            |  $daemonPort (0x$daemonPort%08x)
+            |Python command to execute the daemon was:
+            |  ${command.asScala.mkString(" ")}
+            |Check that you don't have any unexpected modules or libraries in
+            |your PYTHONPATH:
+            |  $pythonPath
+            |Also, check if you have a sitecustomize.py module in your python path,
+            |or in your python installation, that is printing to standard output"""
+          throw new SparkException(exceptionMessage.stripMargin)
+        }
+
+        // Redirect daemon stdout and stderr
+        redirectStreamsToStderr(in, daemon.getErrorStream)
+      } catch {
+        case e: Exception =>
+
+          // If the daemon exists, wait for it to finish and get its stderr
+          val stderr = Option(daemon)
+            .flatMap { d => Utils.getStderr(d, PROCESS_WAIT_TIMEOUT_MS) }
+            .getOrElse("")
+
+          stopDaemon()
+
+          if (stderr != "") {
+            val formattedStderr = stderr.replace("\n", "\n  ")
+            val errorMessage = s"""
+              |Error from executable worker:
+              |  $formattedStderr
+              |PYTHONPATH was:
+              |  $pythonPath
+              |$e"""
+
+            // Append error message from python daemon, but keep original stack trace
+            val wrappedException = new SparkException(errorMessage.stripMargin)
+            wrappedException.setStackTrace(e.getStackTrace)
+            throw wrappedException
+          } else {
+            throw e
+          }
+      }
+
+      // Important: don't close daemon's stdin (daemon.getOutputStream) so it can correctly
+      // detect our disappearance.
+    }
+  }
+}
+
+
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -53,7 +53,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   // as expert-only option, and shouldn't be used before knowing what it means exactly.
 
   // This configuration indicates the module to run the daemon to execute its Python workers.
-  private val daemonModule =
+  protected val daemonModule =
     SparkEnv.get.conf.get(PYTHON_DAEMON_MODULE).map { value =>
       logInfo(
         s"Python daemon module in PySpark is set to [$value] in '${PYTHON_DAEMON_MODULE.key}', " +
@@ -72,13 +72,13 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
       value
     }.getOrElse("pyspark.worker")
 
-  private val authHelper = new SocketAuthHelper(SparkEnv.get.conf)
+  protected val authHelper = new SocketAuthHelper(SparkEnv.get.conf)
 
   @GuardedBy("self")
-  private var daemon: Process = null
+  protected var daemon: Process = null
   val daemonHost = InetAddress.getByAddress(Array(127, 0, 0, 1))
   @GuardedBy("self")
-  private var daemonPort: Int = 0
+  protected var daemonPort: Int = 0
   @GuardedBy("self")
   private val daemonWorkers = new mutable.WeakHashMap[Socket, Int]()
   @GuardedBy("self")
@@ -88,9 +88,9 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   new MonitorThread().start()
 
   @GuardedBy("self")
-  private val simpleWorkers = new mutable.WeakHashMap[Socket, Process]()
+  protected val simpleWorkers = new mutable.WeakHashMap[Socket, Process]()
 
-  private val pythonPath = PythonUtils.mergePythonPaths(
+  protected val pythonPath = PythonUtils.mergePythonPaths(
     PythonUtils.sparkPythonPath,
     envVars.getOrElse("PYTHONPATH", ""),
     sys.env.getOrElse("PYTHONPATH", ""))
@@ -149,7 +149,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   /**
    * Launch a worker by executing worker.py (by default) directly and telling it to connect to us.
    */
-  private def createSimpleWorker(): (Socket, Option[Int]) = {
+  protected def createSimpleWorker(): (Socket, Option[Int]) = {
     var serverSocket: ServerSocket = null
     try {
       serverSocket = new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))
@@ -195,7 +195,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
     null
   }
 
-  private def startDaemon(): Unit = {
+  protected def startDaemon(): Unit = {
     self.synchronized {
       // Is it already running?
       if (daemon != null) {
@@ -282,7 +282,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   /**
    * Redirect the given streams to our stderr in separate threads.
    */
-  private def redirectStreamsToStderr(stdout: InputStream, stderr: InputStream): Unit = {
+  protected def redirectStreamsToStderr(stdout: InputStream, stderr: InputStream): Unit = {
     try {
       new RedirectThread(stdout, System.err, "stdout reader for " + pythonExec).start()
       new RedirectThread(stderr, System.err, "stderr reader for " + pythonExec).start()
@@ -325,7 +325,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
     }
   }
 
-  private def stopDaemon(): Unit = {
+  protected def stopDaemon(): Unit = {
     self.synchronized {
       if (useDaemon) {
         cleanupIdleWorkers()
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/Client.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/Client.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -103,8 +103,13 @@ private class ClientEndpoint(
           sys.env, classPathEntries, libraryPathEntries, javaOpts)
         val driverResourceReqs = ResourceUtils.parseResourceRequirements(conf,
           config.SPARK_DRIVER_PREFIX)
+
+        val pyFilesConf = "spark.python.files"
+        val pyFiles = getProperty(pyFilesConf, conf).map(_.split(",").toSeq).getOrElse(Seq.empty)
+
         val driverDescription = new DriverDescription(
           driverArgs.jarUrl,
+          pyFiles,
           driverArgs.memory,
           driverArgs.cores,
           driverArgs.supervise,
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -21,6 +21,7 @@ import org.apache.spark.resource.ResourceRequirement
 
 private[deploy] case class DriverDescription(
     jarUrl: String,
+    pyFiles: Seq[String],
     mem: Int,
     cores: Int,
     supervise: Boolean,
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/ExecutableRunMode.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/ExecutableRunMode.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -0,0 +1,25 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.deploy
+
+object ExecutableRunMode {
+  val ENV_VARIABLE_NAME = "SPYT_RUN_MODE"
+  val DRIVER = "spyt_driver"
+  val DAEMON = "spyt_daemon"
+  val WORKER = "spyt_worker"
+}
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/ExecutableRunner.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/ExecutableRunner.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.deploy
+
+import java.net.InetAddress
+
+import scala.collection.JavaConverters._
+import scala.collection.mutable.ArrayBuffer
+
+import org.apache.spark.{SparkConf, SparkUserAppException}
+import org.apache.spark.api.python.PythonUtils
+import org.apache.spark.deploy.PythonRunner.formatPaths
+import org.apache.spark.internal.config.PYSPARK_PYTHON
+import org.apache.spark.util.{RedirectThread, Utils}
+
+// Copied from PythonRunner with some small changes
+object ExecutableRunner {
+  def main(args: Array[String]) {
+    val execFile = args(0)
+    val pyFiles = args(1)
+    val execArgs = args.slice(2, args.length)
+    val sparkConf = new SparkConf()
+    val secret = Utils.createSecret(sparkConf)
+
+    val formattedExecFile = PythonRunner.formatPath(execFile)
+    val formattedPyFiles = PythonRunner.resolvePyFiles(formatPaths(pyFiles))
+
+    // Launch a Py4J gateway server for the process to connect to; this will let it see our
+    // Java system properties and such
+    val localhost = InetAddress.getLoopbackAddress()
+    val gatewayServer = new py4j.GatewayServer.GatewayServerBuilder()
+      .authToken(secret)
+      .javaPort(0)
+      .javaAddress(localhost)
+      .callbackClient(py4j.GatewayServer.DEFAULT_PYTHON_PORT, localhost, secret)
+      .build()
+    val thread = new Thread(new Runnable() {
+      override def run(): Unit = Utils.logUncaughtExceptions {
+        gatewayServer.start()
+      }
+    })
+    thread.setName("py4j-gateway-init")
+    thread.setDaemon(true)
+    thread.start()
+
+    // Wait until the gateway server has started, so that we know which port is it bound to.
+    // `gatewayServer.start()` will start a new thread and run the server code there, after
+    // initializing the socket, so the thread started above will end as soon as the server is
+    // ready to serve connections.
+    thread.join()
+
+    // Build up a PYTHONPATH that includes the Spark assembly (where this class is), the
+    // python directories in SPARK_HOME (if set), and any files in the pyFiles argument
+    // All client dependencies should come baked into the executable;
+    // this is done for spyt and pyspark to work without having to include it in each executable
+    val pathElements = new ArrayBuffer[String]
+    pathElements ++= formattedPyFiles
+    pathElements += PythonUtils.sparkPythonPath
+    pathElements += sys.env.getOrElse("PYTHONPATH", "")
+    val pythonPath = PythonUtils.mergePythonPaths(pathElements: _*)
+
+    // Launch Executable
+    val builder = new ProcessBuilder((Seq(formattedExecFile) ++ execArgs).asJava)
+    val env = builder.environment()
+    env.put("PYTHONPATH", pythonPath)
+    env.put(ExecutableRunMode.ENV_VARIABLE_NAME, ExecutableRunMode.DRIVER)
+    // This is equivalent to setting the -u flag; we use it because ipython doesn't support -u:
+    env.put("PYTHONUNBUFFERED", "YES") // value is needed to be set to a non-empty string
+    env.put("PYSPARK_GATEWAY_PORT", "" + gatewayServer.getListeningPort)
+    env.put("PYSPARK_GATEWAY_SECRET", secret)
+    // pass conf spark.pyspark.python to python process, the only way to pass info to
+    // python process is through environment variable.
+    sparkConf.get(PYSPARK_PYTHON).foreach(env.put("PYSPARK_PYTHON", _))
+    sys.env.get("PYTHONHASHSEED").foreach(env.put("PYTHONHASHSEED", _))
+    builder.redirectErrorStream(true) // Ugly but needed for stdout and stderr to synchronize
+    try {
+      val process = builder.start()
+
+      new RedirectThread(process.getInputStream, System.out, "redirect output").start()
+
+      val exitCode = process.waitFor()
+      if (exitCode != 0) {
+        throw new SparkUserAppException(exitCode)
+      }
+    } finally {
+      gatewayServer.shutdown()
+    }
+  }
+}
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -156,7 +156,7 @@ object PythonRunner {
    * not expect a file. This method creates a temporary directory and puts the ".py" files
    * if exist in the given paths.
    */
-  private def resolvePyFiles(pyFiles: Array[String]): Array[String] = {
+  private[deploy] def resolvePyFiles(pyFiles: Array[String]): Array[String] = {
     lazy val dest = Utils.createTempDir(namePrefix = "localPyFiles")
     pyFiles.flatMap { pyFile =>
       // In case of client with submit, the python paths should be set before context
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -269,9 +269,6 @@ private[spark] class SparkSubmit extends Logging {
 
     // Fail fast, the following modes are not supported or applicable
     (clusterManager, deployMode) match {
-      case (STANDALONE, CLUSTER) if args.isPython =>
-        error("Cluster deploy mode is currently not supported for python " +
-          "applications on standalone clusters.")
       case (STANDALONE, CLUSTER) if args.isR =>
         error("Cluster deploy mode is currently not supported for R " +
           "applications on standalone clusters.")
@@ -360,6 +357,12 @@ private[spark] class SparkSubmit extends Logging {
       }
     }
 
+    val ytJars = sparkConf.get("spark.yt.jars", "")
+    args.jars = Option(args.jars).map(mergeFileLists(_, ytJars)).getOrElse(ytJars)
+
+    val ytPyFiles = sparkConf.get("spark.yt.pyFiles", "")
+    args.pyFiles = Option(args.pyFiles).map(mergeFileLists(_, ytPyFiles)).getOrElse(ytPyFiles)
+
     // Resolve glob path for different resources.
     args.jars = Option(args.jars).map(resolveGlobPaths(_, hadoopConf)).orNull
     args.files = Option(args.files).map(resolveGlobPaths(_, hadoopConf)).orNull
@@ -469,7 +472,7 @@ private[spark] class SparkSubmit extends Logging {
 
     // At this point, we have attempted to download all remote resources.
     // Now we try to resolve the main class if our primary resource is a JAR.
-    if (args.mainClass == null && !args.isPython && !args.isR) {
+    if (args.mainClass == null && !args.isPython && !args.isR && !args.isExecutable) {
       try {
         val uri = new URI(
           Option(localPrimaryResource).getOrElse(args.primaryResource)
@@ -509,6 +512,33 @@ private[spark] class SparkSubmit extends Logging {
     if (deployMode == CLIENT && clusterManager != YARN) {
       // The YARN backend handles python files differently, so don't merge the lists.
       args.files = mergeFileLists(args.files, args.pyFiles)
+
+      val submitPyFiles = sparkConf.getOption("spark.submit.pyFiles").orNull
+      Option(mergeFileLists(submitPyFiles, args.pyFiles)).foreach(
+        sparkConf.set("spark.submit.pyFiles", _)
+      )
+    }
+
+    if (args.isExecutable && clusterManager == STANDALONE && deployMode == CLUSTER) {
+      sparkConf.set("spark.yt.isExecutable", "true")
+      sparkConf.set("spark.yt.executableResource", args.primaryResource)
+      args.mainClass = EXECUTABLE_RUNNER_SUBMIT_CLASS
+      val submitPyFiles = sparkConf.getOption("spark.submit.pyFiles").orNull
+      // make sure our executable is not in spark.submit.pyFiles: bad things happen when it is
+      sparkConf.set("spark.submit.pyFiles", mergeFileLists(submitPyFiles, args.pyFiles))
+      args.pyFiles = mergeFileLists(args.pyFiles, args.primaryResource)
+      args.files = mergeFileLists(args.files, args.pyFiles)
+      args.childArgs = ArrayBuffer("{{USER_JAR}}", "{{PY_FILES}}") ++ args.childArgs
+      sparkConf.set("spark.python.files", Option(args.pyFiles).getOrElse(""))
+    }
+
+    if (args.isPython && clusterManager == STANDALONE && deployMode == CLUSTER) {
+      args.mainClass = "org.apache.spark.deploy.PythonRunner"
+      args.childArgs = ArrayBuffer("{{USER_JAR}}", "{{PY_FILES}}") ++ args.childArgs
+      args.files = mergeFileLists(args.files, args.pyFiles)
+      sparkConf.set("spark.python.files", Option(args.pyFiles).getOrElse(""))
+      val submitPyFiles = sparkConf.getOption("spark.submit.pyFiles").orNull
+      sparkConf.set("spark.submit.pyFiles", mergeFileLists(submitPyFiles, args.pyFiles))
     }
 
     if (localPyFiles != null) {
@@ -1017,6 +1047,8 @@ object SparkSubmit extends CommandLineUtils with Logging {
   private[deploy] val STANDALONE_CLUSTER_SUBMIT_CLASS = classOf[ClientApp].getName()
   private[deploy] val KUBERNETES_CLUSTER_SUBMIT_CLASS =
     "org.apache.spark.deploy.k8s.submit.KubernetesClientApplication"
+  private[deploy] val EXECUTABLE_RUNNER_SUBMIT_CLASS =
+    "org.apache.spark.deploy.ExecutableRunner"
 
   override def main(args: Array[String]): Unit = {
     val submit = new SparkSubmit() {
@@ -1056,7 +1088,7 @@ object SparkSubmit extends CommandLineUtils with Logging {
    * Return whether the given primary resource represents a user jar.
    */
   private[deploy] def isUserJar(res: String): Boolean = {
-    !isShell(res) && !isPython(res) && !isInternal(res) && !isR(res)
+    res != null && res.endsWith(".jar")
   }
 
   /**
@@ -1094,6 +1126,13 @@ object SparkSubmit extends CommandLineUtils with Logging {
     res != null && (res.endsWith(".R") || res.endsWith(".r")) || res == SPARKR_SHELL
   }
 
+  /**
+   * Return whether the given primary resource is an executable.
+   */
+  private[deploy] def isExecutable(res: String): Boolean = {
+    !isShell(res) && !isPython(res) && !isInternal(res) && !isR(res) && !isUserJar(res)
+  }
+
   private[deploy] def isInternal(res: String): Boolean = {
     res == SparkLauncher.NO_RESOURCE
   }
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -67,6 +67,7 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
   var packagesExclusions: String = null
   var verbose: Boolean = false
   var isPython: Boolean = false
+  var isExecutable: Boolean = false
   var pyFiles: String = null
   var isR: Boolean = false
   var action: SparkSubmitAction = null
@@ -112,6 +113,7 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
   // Remove keys that don't start with "spark." from `sparkProperties`.
   ignoreNonSparkProperties()
   // Use `sparkProperties` map along with env vars to fill in any missing parameters
+
   loadEnvironmentArguments()
 
   useRest = sparkProperties.getOrElse("spark.master.rest.enabled", "false").toBoolean
@@ -243,7 +245,7 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
       printUsageAndExit(-1)
     }
     if (primaryResource == null) {
-      error("Must specify a primary resource (JAR or Python or R file)")
+      error("Must specify a primary resource (JAR or Python or R or an executable file)")
     }
     if (driverMemory != null
         && Try(JavaUtils.byteStringAsBytes(driverMemory)).getOrElse(-1L) <= 0) {
@@ -463,13 +465,15 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
     }
 
     primaryResource =
-      if (!SparkSubmit.isShell(opt) && !SparkSubmit.isInternal(opt)) {
+      if (!SparkSubmit.isShell(opt) && !SparkSubmit.isInternal(opt) &&
+        !SparkSubmit.isExecutable(opt)) {
         Utils.resolveURI(opt).toString
       } else {
         opt
       }
     isPython = SparkSubmit.isPython(opt)
     isR = SparkSubmit.isR(opt)
+    isExecutable = SparkSubmit.isExecutable(opt)
     false
   }
 
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -156,6 +156,8 @@ private[rest] class StandaloneSubmitRequestServlet(
     // Filter SPARK_LOCAL_(IP|HOSTNAME) environment variables from being set on the remote system.
     val environmentVariables =
       request.environmentVariables.filterNot(x => x._1.matches("SPARK_LOCAL_(IP|HOSTNAME)"))
+    val pyFiles = sparkProperties.get("spark.python.files")
+      .map(_.split(",").toSeq).getOrElse(Seq.empty)
 
     // Construct driver description
     val conf = new SparkConf(false)
@@ -178,7 +180,7 @@ private[rest] class StandaloneSubmitRequestServlet(
     val driverResourceReqs = ResourceUtils.parseResourceRequirements(conf,
       config.SPARK_DRIVER_PREFIX)
     new DriverDescription(
-      appResource, actualDriverMemory, actualDriverCores, actualSuperviseDriver, command,
+      appResource, pyFiles, actualDriverMemory, actualDriverCores, actualSuperviseDriver, command,
       driverResourceReqs)
   }
 
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -155,20 +155,28 @@ private[deploy] class DriverRunner(
    * Will throw an exception if there are errors downloading the jar.
    */
   private def downloadUserJar(driverDir: File): String = {
-    val jarFileName = new URI(driverDesc.jarUrl).getPath.split("/").last
-    val localJarFile = new File(driverDir, jarFileName)
+    downloadFile(driverDesc.jarUrl, driverDir)
+  }
+
+  private def downloadPyFiles(driverDir: File): Seq[String] = {
+    driverDesc.pyFiles.map(downloadFile(_, driverDir))
+  }
+
+  private def downloadFile(sourceUrl: String, destDir: File): String = {
+    val fileName = new URI(sourceUrl).getPath.split("/").last
+    val localJarFile = new File(destDir, fileName)
     if (!localJarFile.exists()) { // May already exist if running multiple workers on one node
-      logInfo(s"Copying user jar ${driverDesc.jarUrl} to $localJarFile")
+      logInfo(s"Copying user file $sourceUrl to $localJarFile")
       Utils.fetchFile(
-        driverDesc.jarUrl,
-        driverDir,
+        sourceUrl,
+        destDir,
         conf,
         SparkHadoopUtil.get.newConfiguration(conf),
         System.currentTimeMillis(),
         useCache = false)
       if (!localJarFile.exists()) { // Verify copy succeeded
         throw new IOException(
-          s"Can not find expected jar $jarFileName which should have been loaded in $driverDir")
+          s"Can not find expected file $fileName which should have been loaded in $destDir")
       }
     }
     localJarFile.getAbsolutePath
@@ -178,10 +186,12 @@ private[deploy] class DriverRunner(
     val driverDir = createWorkingDirectory()
     val localJarFilename = downloadUserJar(driverDir)
     val resourceFileOpt = prepareResourcesFile(SPARK_DRIVER_PREFIX, resources, driverDir)
+    val localPyFiles = downloadPyFiles(driverDir)
 
     def substituteVariables(argument: String): String = argument match {
       case "{{WORKER_URL}}" => workerUrl
       case "{{USER_JAR}}" => localJarFilename
+      case "{{PY_FILES}}" => localPyFiles.mkString(",")
       case other => other
     }
 
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -83,11 +83,11 @@ object DriverWrapper extends Logging {
       ivyProperties.ivyRepoPath, Option(ivyProperties.ivySettingsPath))
     val jars = {
       val jarsProp = sys.props.get(config.JARS.key).orNull
+      val ytJarsProp = sys.props.get("spark.yt.jars").orNull
       if (resolvedMavenCoordinates.nonEmpty) {
-        DependencyUtils.mergeFileLists(jarsProp,
-          DependencyUtils.mergeFileLists(resolvedMavenCoordinates: _*))
+        DependencyUtils.mergeFileLists(jarsProp +: ytJarsProp +: resolvedMavenCoordinates : _*)
       } else {
-        jarsProp
+        DependencyUtils.mergeFileLists(jarsProp, ytJarsProp)
       }
     }
     val localJars = DependencyUtils.resolveAndDownloadJars(jars, userJar, sparkConf, hadoopConf)
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -18,6 +18,7 @@
 package org.apache.spark.deploy.worker
 
 import java.io._
+import java.net.URI
 import java.nio.charset.StandardCharsets
 
 import scala.collection.JavaConverters._
@@ -25,7 +26,7 @@ import scala.collection.JavaConverters._
 import com.google.common.io.Files
 
 import org.apache.spark.{SecurityManager, SparkConf}
-import org.apache.spark.deploy.{ApplicationDescription, ExecutorState}
+import org.apache.spark.deploy.{ApplicationDescription, ExecutorState, SparkHadoopUtil}
 import org.apache.spark.deploy.DeployMessages.ExecutorStateChanged
 import org.apache.spark.deploy.StandaloneResourceUtils.prepareResourcesFile
 import org.apache.spark.internal.Logging
@@ -89,6 +90,26 @@ private[deploy] class ExecutorRunner(
       killProcess(Some("Worker shutting down")) }
   }
 
+  private def downloadFile(sourceUrl: String, destDir: File): String = {
+    val fileName = new URI(sourceUrl).getPath.split("/").last
+    val localJarFile = new File(destDir, fileName)
+    if (!localJarFile.exists()) { // May already exist if running multiple workers on one node
+      logInfo(s"Copying user file $sourceUrl to $localJarFile")
+      Utils.fetchFile(
+        sourceUrl,
+        destDir,
+        conf,
+        SparkHadoopUtil.get.newConfiguration(conf),
+        System.currentTimeMillis(),
+        useCache = false)
+      if (!localJarFile.exists()) { // Verify copy succeeded
+        throw new IOException(
+          s"Can not find expected file $fileName which should have been loaded in $destDir")
+      }
+    }
+    localJarFile.getAbsolutePath
+  }
+
   /**
    * Kill executor process, wait for exit and notify worker to update resource status.
    *
@@ -142,6 +163,18 @@ private[deploy] class ExecutorRunner(
     case other => other
   }
 
+  private def substituteFiles(opt: String): String = {
+    // TODO make more general
+    if (opt.contains("yt:///")) {
+      val name :: files :: Nil = opt.split("=", 2).toList
+      val downloadedFiles = files.split(",")
+        .map(file => s"file://${downloadFile(file, executorDir)}")
+      val newOpt = s"$name=${downloadedFiles.mkString(",")}"
+      logInfo(s"Substitute $opt with $newOpt")
+      newOpt
+    } else opt
+  }
+
   /**
    * Download and run the executor described in our ApplicationDescription
    */
@@ -151,8 +184,8 @@ private[deploy] class ExecutorRunner(
       // Launch the process
       val arguments = appDesc.command.arguments ++ resourceFileOpt.map(f =>
         Seq("--resourcesFile", f.getAbsolutePath)).getOrElse(Seq.empty)
-      val subsOpts = appDesc.command.javaOpts.map {
-        Utils.substituteAppNExecIds(_, appId, execId.toString)
+      val subsOpts = appDesc.command.javaOpts.map { opt =>
+        substituteFiles(Utils.substituteAppNExecIds(opt, appId, execId.toString))
       }
       val subsCommand = appDesc.command.copy(arguments = arguments, javaOpts = subsOpts)
       val builder = CommandUtils.buildProcessBuilder(subsCommand, new SecurityManager(conf),
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/internal/config/package.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/internal/config/package.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -1895,6 +1895,12 @@ package object config {
     .toSequence
     .createWithDefault(Nil)
 
+  private[spark] val YT_JARS = ConfigBuilder("spark.yt.jars")
+    .version("0.9.0")
+    .stringConf
+    .toSequence
+    .createWithDefault(Nil)
+
   private[spark] val FILES = ConfigBuilder("spark.files")
     .version("1.0.0")
     .stringConf
--- taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/util/Utils.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/main/scala/org/apache/spark/util/Utils.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -2772,7 +2772,7 @@ private[spark] object Utils extends Logging {
    * has its own mechanism to distribute jars.
    */
   def getUserJars(conf: SparkConf): Seq[String] = {
-    conf.get(JARS).filter(_.nonEmpty)
+    conf.get(JARS).filter(_.nonEmpty) ++ conf.get(YT_JARS).filter(_.nonEmpty)
   }
 
   /**
--- taxi/dmp/spark/spark/core/src/test/scala/org/apache/spark/deploy/DeployTestUtils.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/test/scala/org/apache/spark/deploy/DeployTestUtils.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -46,7 +46,7 @@ private[deploy] object DeployTestUtils {
   )
 
   def createDriverDesc(): DriverDescription =
-    new DriverDescription("hdfs://some-dir/some.jar", 100, 3, false, createDriverCommand())
+    new DriverDescription("hdfs://some-dir/some.jar", Nil, 100, 3, false, createDriverCommand())
 
   def createDriverInfo(): DriverInfo = {
     val dDesc = createDriverDesc().copy(resourceReqs = createResourceRequirement)
--- taxi/dmp/spark/spark/core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -157,7 +157,7 @@ class SparkSubmitSuite
       "some",
       "--weird", "args")
     val appArgs = new SparkSubmitArguments(clArgs)
-    appArgs.childArgs should be (Seq("some", "--weird", "args"))
+    appArgs.childArgs should be(Seq("some", "--weird", "args"))
   }
 
   test("handles arguments to user program with name collision") {
@@ -930,6 +930,22 @@ class SparkSubmitSuite
     }
   }
 
+  test("resolves local executable correctly") {
+    val execPath = "/path/to/local/executable"
+    val args = Seq(execPath, "some", "--weird", "args")
+    val appArgs = new SparkSubmitArguments(args)
+
+    appArgs.isExecutable should be (true)
+    appArgs.isPython should be (false)
+    appArgs.isR should be (false)
+
+    val (childArgs, classpath, conf, mainClass) = submit.prepareSubmitEnvironment(appArgs)
+    mainClass should be (SparkSubmit.EXECUTABLE_RUNNER_SUBMIT_CLASS)
+    childArgs should be(args)
+    conf.getBoolean("spark.yt.isExecutable", false) should be(true)
+    conf.get("spark.yt.executableResource") should be (execPath)
+  }
+
   test("user classpath first in driver") {
     val systemJar = TestUtils.createJarWithFiles(Map("test.resource" -> "SYSTEM"))
     val userJar = TestUtils.createJarWithFiles(Map("test.resource" -> "USER"))
--- taxi/dmp/spark/spark/core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -197,6 +197,7 @@ class MasterSuite extends SparkFunSuite
       id = "test_driver",
       desc = new DriverDescription(
         jarUrl = "",
+        pyFiles = Nil,
         mem = 0,
         cores = 0,
         supervise = false,
@@ -256,6 +257,7 @@ class MasterSuite extends SparkFunSuite
       id = "test_driver",
       desc = new DriverDescription(
         jarUrl = "",
+        pyFiles = Nil,
         mem = 1024,
         cores = 1,
         supervise = false,
--- taxi/dmp/spark/spark/core/src/test/scala/org/apache/spark/deploy/worker/DriverRunnerTest.scala	(13ab3653f7d9332622324f9202aac0e7b6b3c228)
+++ taxi/dmp/spark/spark/core/src/test/scala/org/apache/spark/deploy/worker/DriverRunnerTest.scala	(5f95d55d0aeec435d71fc82fdb11e19f3df41fe8)
@@ -35,7 +35,7 @@ import org.apache.spark.util.Clock
 class DriverRunnerTest extends SparkFunSuite {
   private def createDriverRunner() = {
     val command = new Command("mainClass", Seq(), Map(), Seq(), Seq(), Seq())
-    val driverDescription = new DriverDescription("jarUrl", 512, 1, true, command)
+    val driverDescription = new DriverDescription("jarUrl", Nil, 512, 1, true, command)
     val conf = new SparkConf()
     val worker = mock(classOf[RpcEndpointRef])
     doNothing().when(worker).send(any())
