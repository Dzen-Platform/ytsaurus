# Stubs for pyspark.sql.readwriter (Python 3.5)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from typing import Any, Optional
from pyspark.sql.types import *

class OptionUtils: ...

class DataFrameReader(OptionUtils):
    def __init__(self, spark) -> None: ...
    def format(self, source): ...
    def schema(self, schema): ...
    def option(self, key, value): ...
    def options(self, **options): ...
    def load(self, path: Optional[Any] = ..., format: Optional[Any] = ..., schema: Optional[Any] = ..., **options): ...
    def json(self, path, schema: Optional[Any] = ..., primitivesAsString: Optional[Any] = ..., prefersDecimal: Optional[Any] = ..., allowComments: Optional[Any] = ..., allowUnquotedFieldNames: Optional[Any] = ..., allowSingleQuotes: Optional[Any] = ..., allowNumericLeadingZero: Optional[Any] = ..., allowBackslashEscapingAnyCharacter: Optional[Any] = ..., mode: Optional[Any] = ..., columnNameOfCorruptRecord: Optional[Any] = ..., dateFormat: Optional[Any] = ..., timestampFormat: Optional[Any] = ...): ...
    def table(self, tableName): ...
    def parquet(self, *paths): ...
    def text(self, paths): ...
    def csv(self, path, schema: Optional[Any] = ..., sep: Optional[Any] = ..., encoding: Optional[Any] = ..., quote: Optional[Any] = ..., escape: Optional[Any] = ..., comment: Optional[Any] = ..., header: Optional[Any] = ..., inferSchema: Optional[Any] = ..., ignoreLeadingWhiteSpace: Optional[Any] = ..., ignoreTrailingWhiteSpace: Optional[Any] = ..., nullValue: Optional[Any] = ..., nanValue: Optional[Any] = ..., positiveInf: Optional[Any] = ..., negativeInf: Optional[Any] = ..., dateFormat: Optional[Any] = ..., timestampFormat: Optional[Any] = ..., maxColumns: Optional[Any] = ..., maxCharsPerColumn: Optional[Any] = ..., maxMalformedLogPerPartition: Optional[Any] = ..., mode: Optional[Any] = ...): ...
    def orc(self, path): ...
    def jdbc(self, url, table, column: Optional[Any] = ..., lowerBound: Optional[Any] = ..., upperBound: Optional[Any] = ..., numPartitions: Optional[Any] = ..., predicates: Optional[Any] = ..., properties: Optional[Any] = ...): ...

class DataFrameWriter(OptionUtils):
    def __init__(self, df) -> None: ...
    def mode(self, saveMode): ...
    def format(self, source): ...
    def option(self, key, value): ...
    def options(self, **options): ...
    def partitionBy(self, *cols): ...
    def save(self, path: Optional[Any] = ..., format: Optional[Any] = ..., mode: Optional[Any] = ..., partitionBy: Optional[Any] = ..., **options): ...
    def insertInto(self, tableName, overwrite: bool = ...): ...
    def saveAsTable(self, name, format: Optional[Any] = ..., mode: Optional[Any] = ..., partitionBy: Optional[Any] = ..., **options): ...
    def json(self, path, mode: Optional[Any] = ..., compression: Optional[Any] = ..., dateFormat: Optional[Any] = ..., timestampFormat: Optional[Any] = ...): ...
    def parquet(self, path, mode: Optional[Any] = ..., partitionBy: Optional[Any] = ..., compression: Optional[Any] = ...): ...
    def text(self, path, compression: Optional[Any] = ...): ...
    def csv(self, path, mode: Optional[Any] = ..., compression: Optional[Any] = ..., sep: Optional[Any] = ..., quote: Optional[Any] = ..., escape: Optional[Any] = ..., header: Optional[Any] = ..., nullValue: Optional[Any] = ..., escapeQuotes: Optional[Any] = ..., quoteAll: Optional[Any] = ..., dateFormat: Optional[Any] = ..., timestampFormat: Optional[Any] = ...): ...
    def orc(self, path, mode: Optional[Any] = ..., partitionBy: Optional[Any] = ..., compression: Optional[Any] = ...): ...
    def jdbc(self, url, table, mode: Optional[Any] = ..., properties: Optional[Any] = ...): ...

