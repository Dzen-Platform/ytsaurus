# Stubs for pyspark.context (Python 3.5)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from typing import Any, Optional

class SparkContext:
    PACKAGE_EXTENSIONS = ...  # type: Any
    def __init__(self, master: Optional[Any] = ..., appName: Optional[Any] = ..., sparkHome: Optional[Any] = ..., pyFiles: Optional[Any] = ..., environment: Optional[Any] = ..., batchSize: int = ..., serializer: Any = ..., conf: Optional[Any] = ..., gateway: Optional[Any] = ..., jsc: Optional[Any] = ..., profiler_cls: Any = ...) -> None: ...
    def __getnewargs__(self): ...
    def __enter__(self): ...
    def __exit__(self, type, value, trace): ...
    @classmethod
    def getOrCreate(cls, conf: Optional[Any] = ...): ...
    def setLogLevel(self, logLevel): ...
    @classmethod
    def setSystemProperty(cls, key, value): ...
    @property
    def version(self): ...
    @property
    def applicationId(self): ...
    @property
    def uiWebUrl(self): ...
    @property
    def startTime(self): ...
    @property
    def defaultParallelism(self): ...
    @property
    def defaultMinPartitions(self): ...
    def stop(self): ...
    def emptyRDD(self): ...
    def range(self, start, end: Optional[Any] = ..., step: int = ..., numSlices: Optional[Any] = ...): ...
    def parallelize(self, c, numSlices: Optional[Any] = ...): ...
    def pickleFile(self, name, minPartitions: Optional[Any] = ...): ...
    def textFile(self, name, minPartitions: Optional[Any] = ..., use_unicode: bool = ...): ...
    def wholeTextFiles(self, path, minPartitions: Optional[Any] = ..., use_unicode: bool = ...): ...
    def binaryFiles(self, path, minPartitions: Optional[Any] = ...): ...
    def binaryRecords(self, path, recordLength): ...
    def sequenceFile(self, path, keyClass: Optional[Any] = ..., valueClass: Optional[Any] = ..., keyConverter: Optional[Any] = ..., valueConverter: Optional[Any] = ..., minSplits: Optional[Any] = ..., batchSize: int = ...): ...
    def newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter: Optional[Any] = ..., valueConverter: Optional[Any] = ..., conf: Optional[Any] = ..., batchSize: int = ...): ...
    def newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter: Optional[Any] = ..., valueConverter: Optional[Any] = ..., conf: Optional[Any] = ..., batchSize: int = ...): ...
    def hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter: Optional[Any] = ..., valueConverter: Optional[Any] = ..., conf: Optional[Any] = ..., batchSize: int = ...): ...
    def hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter: Optional[Any] = ..., valueConverter: Optional[Any] = ..., conf: Optional[Any] = ..., batchSize: int = ...): ...
    def union(self, rdds): ...
    def broadcast(self, value): ...
    def accumulator(self, value, accum_param: Optional[Any] = ...): ...
    def addFile(self, path, recursive: bool = ...): ...
    def addPyFile(self, path): ...
    def setCheckpointDir(self, dirName): ...
    def setJobGroup(self, groupId, description, interruptOnCancel: bool = ...): ...
    def setLocalProperty(self, key, value): ...
    def getLocalProperty(self, key): ...
    def sparkUser(self): ...
    def cancelJobGroup(self, groupId): ...
    def cancelAllJobs(self): ...
    def statusTracker(self): ...
    def runJob(self, rdd, partitionFunc, partitions: Optional[Any] = ..., allowLocal: bool = ...): ...
    def show_profiles(self): ...
    def dump_profiles(self, path): ...
    def getConf(self): ...
