# Stubs for pyspark.streaming.dstream (Python 3.5)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from typing import Any, Optional

class DStream:
    is_cached = ...  # type: bool
    is_checkpointed = ...  # type: bool
    def __init__(self, jdstream, ssc, jrdd_deserializer) -> None: ...
    def context(self): ...
    def count(self): ...
    def filter(self, f): ...
    def flatMap(self, f, preservesPartitioning: bool = ...): ...
    def map(self, f, preservesPartitioning: bool = ...): ...
    def mapPartitions(self, f, preservesPartitioning: bool = ...): ...
    def mapPartitionsWithIndex(self, f, preservesPartitioning: bool = ...): ...
    def reduce(self, func): ...
    def reduceByKey(self, func, numPartitions: Optional[Any] = ...): ...
    def combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions: Optional[Any] = ...): ...
    def partitionBy(self, numPartitions, partitionFunc: Any = ...): ...
    def foreachRDD(self, func): ...
    def pprint(self, num: int = ...): ...
    def mapValues(self, f): ...
    def flatMapValues(self, f): ...
    def glom(self): ...
    def cache(self): ...
    def persist(self, storageLevel): ...
    def checkpoint(self, interval): ...
    def groupByKey(self, numPartitions: Optional[Any] = ...): ...
    def countByValue(self): ...
    def saveAsTextFiles(self, prefix, suffix: Optional[Any] = ...): ...
    def transform(self, func): ...
    def transformWith(self, func, other, keepSerializer: bool = ...): ...
    def repartition(self, numPartitions): ...
    def union(self, other): ...
    def cogroup(self, other, numPartitions: Optional[Any] = ...): ...
    def join(self, other, numPartitions: Optional[Any] = ...): ...
    def leftOuterJoin(self, other, numPartitions: Optional[Any] = ...): ...
    def rightOuterJoin(self, other, numPartitions: Optional[Any] = ...): ...
    def fullOuterJoin(self, other, numPartitions: Optional[Any] = ...): ...
    def slice(self, begin, end): ...
    def window(self, windowDuration, slideDuration: Optional[Any] = ...): ...
    def reduceByWindow(self, reduceFunc, invReduceFunc, windowDuration, slideDuration): ...
    def countByWindow(self, windowDuration, slideDuration): ...
    def countByValueAndWindow(self, windowDuration, slideDuration, numPartitions: Optional[Any] = ...): ...
    def groupByKeyAndWindow(self, windowDuration, slideDuration, numPartitions: Optional[Any] = ...): ...
    def reduceByKeyAndWindow(self, func, invFunc, windowDuration, slideDuration: Optional[Any] = ..., numPartitions: Optional[Any] = ..., filterFunc: Optional[Any] = ...): ...
    def updateStateByKey(self, updateFunc, numPartitions: Optional[Any] = ..., initialRDD: Optional[Any] = ...): ...

class TransformedDStream(DStream):
    is_cached = ...  # type: bool
    is_checkpointed = ...  # type: bool
    func = ...  # type: Any
    prev = ...  # type: Any
    def __init__(self, prev, func) -> None: ...
