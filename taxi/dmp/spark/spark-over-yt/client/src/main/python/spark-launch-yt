#!/usr/bin/env python

import argparse
import os
import re

import configparser
import yt
from yt.wrapper.cypress_commands import create, exists, list
from yt.wrapper.operation_commands import TimeWatcher, process_operation_unsuccesful_finish_state
from yt.wrapper.run_operation_commands import run_operation
from yt.wrapper.spec_builders import VanillaSpecBuilder

units = {"gb": 1024 * 1024 * 1024, "mb": 1024 * 1024, "kb": 1024, "bb": 1, "b": 1}
config = configparser.ConfigParser()
config.read("{}/conf/spark-launch.ini".format(os.getenv("SPARK_HOME")))


def parse_memory(memory_str):
    if memory_str is None:
        return None
    m = re.match("(\d+)(.*)", memory_str)
    value = long(m.group(1))
    unit = m.group(2).lower().strip()
    if len(unit) <= 1:
        unit = unit + "b"
    return value * units[unit]


def jmx_opts(port):
    return "-Dcom.sun.management.jmxremote " \
           "-Dcom.sun.management.jmxremote.port={} " \
           "-Dcom.sun.management.jmxremote.authenticate=false " \
           "-Dcom.sun.management.jmxremote.ssl=false".format(port)


def get_config(name):
    return config["DEFAULT"][name]


def wait_master_start():
    for state in op.get_state_monitor(TimeWatcher(1.0, 1.0, 0.0)):
        if state.is_running() and exists("{0}/instances/{1}/operation/{2}".format(working_dir, args.id, op.id)):
            return op
        elif state.is_unsuccessfully_finished():
            process_operation_unsuccesful_finish_state(op, state)
        else:
            op.printer(state)


parser = argparse.ArgumentParser(description="Spark over YT")
parser.add_argument("--id", required=True)
parser.add_argument("--worker-cores", required=True, type=int)
parser.add_argument("--worker-memory", required=True)
parser.add_argument("--worker-num", required=True, type=int)
parser.add_argument("--proxy", required=False)
parser.add_argument("--working-dir", required=False)
parser.add_argument("--master-memory-limit", required=False)
parser.add_argument("--pool", required=False)

args = parser.parse_args()

user = os.getenv("USER")
proxy = args.proxy or os.getenv("YT_PROXY")
recovery_opts = "-Dspark.deploy.recoveryMode=CUSTOM " \
                "-Dspark.deploy.recoveryMode.factory=org.apache.spark.deploy.master.YtRecoveryModeFactory " \
                "-Dspark.deploy.yt.path=/home/sashbel/master"
worker_opts = "-Dspark.worker.cleanup.enabled=true " \
              "-Dspark.shuffle.service.enabled=true " \
              "-Dspark.hadoop.fs.yt.impl=ru.yandex.spark.yt.format.YtFileSystem " \
              "-Dspark.port.maxRetries={0} " \
              "-Dspark.shuffle.service.port={1}" \
    .format(get_config("PortMaxRetries"), get_config("ShuffleServicePort"))
master_opts = "-Dspark.port.maxRetries={0} " \
              "-Dspark.hadoop.fs.yt.impl=ru.yandex.spark.yt.format.YtFileSystem " \
              "-Dspark.master.rest.enabled=true " \
              "-Dspark.master.rest.port={1}" \
    .format(get_config("PortMaxRetries"), get_config("StartPort"))

working_dir = args.working_dir or "//home/{0}/spark-tmp".format(user)
master_memory_limit = parse_memory(args.master_memory_limit) or 2 * 1024 * 1024 * 1024
worker_memory_add = 2 * 1024 * 1024 * 1024

unpack_tar = "tar --warning=no-unknown-keyword -xf {0}.tgz".format(get_config("SparkName"))
run_launcher = "/opt/jdk8/bin/java -Xmx512m -cp {0}".format(get_config("SparkLauncherName"))

file_paths = ["{0}/{1}.tgz".format(get_config("SparkYtBasePath"), get_config("SparkName")),
              "{0}/{1}".format(get_config("SparkYtBasePath"), get_config("SparkLauncherName"))]

layer_paths = ["//home/sashbel/delta/jdk/layer_with_jdk_lastest.tar.gz",
               "//porto_layers/base/xenial/porto_layer_search_ubuntu_xenial_app_lastest.tar.gz"]

master_command = "{0} && {1} ru.yandex.spark.launcher.MasterLauncher --id {2} --operation-id $YT_OPERATION_ID " \
                 "--port {3} --web-ui-port {3} --opts \"'{4}'\"" \
    .format(unpack_tar, run_launcher, args.id,
            get_config("StartPort"), master_opts)

worker_command = "{0} && {1} ru.yandex.spark.launcher.WorkerLauncher --id {2} --cores {3} --memory {4} " \
                 "--port {5} --web-ui-port {5} --opts \"'{6}'\"" \
    .format(unpack_tar, run_launcher, args.id, args.worker_cores, args.worker_memory,
            get_config("StartPort"), worker_opts)

history_path = "{0}/logs/{1}".format(working_dir, args.id)
history_command = "{0} && {1} ru.yandex.spark.launcher.HistoryServerLauncher --id {2} --log-path yt:/{3} " \
                 "--port {4} --opts \"'{5}'\"" \
    .format(unpack_tar, run_launcher, args.id, history_path, get_config("StartPort"), worker_opts)

environment = {
    "JAVA_HOME": "/opt/jdk8",
    "SPARK_HOME": get_config("SparkName"),
    "YT_PROXY": proxy,
    "SPARK_DISCOVERY_PATH": "{0}/instances".format(working_dir)
}

operation_spec = {
    "stderr_table_path": "{0}/logs/stderr_{1}".format(working_dir, args.id),
    "pool": args.pool or user
}

task_spec = {
    "restart_completed_jobs": True
}

token = os.getenv("YT_TOKEN")
if token is None:
    with open(os.getenv("HOME") + "/.yt/token") as f:
        token = f.readline().strip()

secure_vault = {
    "YT_USER": user,
    "YT_TOKEN": token
}

spec_builder = \
    VanillaSpecBuilder() \
        .begin_task("master") \
            .job_count(1) \
            .file_paths(file_paths) \
            .command(master_command) \
            .memory_limit(master_memory_limit) \
            .memory_reserve_factor(1.0) \
            .cpu_limit(2) \
            .environment(environment) \
            .layer_paths(layer_paths) \
            .spec(task_spec) \
        .end_task() \
        .begin_task("history") \
            .job_count(1) \
            .file_paths(file_paths) \
            .command(history_command) \
            .memory_limit(master_memory_limit) \
            .memory_reserve_factor(1.0) \
            .cpu_limit(1) \
            .environment(environment) \
            .layer_paths(layer_paths) \
            .spec(task_spec) \
        .end_task() \
        .begin_task("workers") \
            .job_count(args.worker_num) \
            .file_paths(file_paths) \
            .command(worker_command) \
            .memory_limit(parse_memory(args.worker_memory) + worker_memory_add) \
            .memory_reserve_factor(1.0) \
            .cpu_limit(args.worker_cores) \
            .disk_space_limit(parse_memory("100G")) \
            .environment(environment) \
            .layer_paths(layer_paths) \
            .spec(task_spec) \
        .end_task() \
        .secure_vault(secure_vault) \
        .max_failed_job_count(1) \
        .max_stderr_count(150) \
        .spec(operation_spec)

yt.wrapper.config.config['proxy']['url'] = proxy
create("map_node", "{0}/logs".format(working_dir), recursive=True, ignore_existing=True)
create("map_node", history_path, recursive=True, ignore_existing=True)
op = run_operation(spec_builder, sync=False)
wait_master_start()
master_address = list("{0}/instances/{1}/webui".format(working_dir, args.id))[0]

print("Spark Master's Web UI: http://{0}".format(master_address))
