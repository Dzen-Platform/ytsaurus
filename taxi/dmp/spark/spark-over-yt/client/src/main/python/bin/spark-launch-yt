#!/usr/bin/env python

import sys
import os
import subprocess

spark_home = os.environ.get("SPARK_HOME")
if spark_home is None:
    try:
        bin_dir = os.path.dirname(os.path.realpath(__file__))
        spark_home = subprocess.check_output("find_spark_home.py").strip().decode("utf-8")
    except:
        raise RuntimeError("Unable to find SPARK_HOME automatically from {}".format(os.path.realpath(__file__)))

try:
    from yt.wrapper import YtClient
    from yt.wrapper.cli_helpers import ParseStructuredArgument
    from yt.wrapper.http_helpers import get_user_name
    from spyt.standalone import start_spark_cluster, find_spark_cluster, SparkDefaultArguments, SpytEnablers
    from spyt import utils as spark_utils
    sys.path.insert(0, os.path.join(spark_home, "bin", "python"))
except Exception as err:
    print("Spyt modules not found, falling back to pysark: %s " % err)
    sys.path.insert(0, os.path.join(spark_home, "bin", "python"))
    from yt.wrapper import YtClient
    from yt.wrapper.cli_helpers import ParseStructuredArgument
    from yt.wrapper.http_helpers import get_user_name
    from spyt.standalone import start_spark_cluster, find_spark_cluster, SparkDefaultArguments, SpytEnablers
    from spyt import utils as spark_utils


def main():
    parser = spark_utils.get_default_arg_parser(description="Spark over YT")

    parser.add_argument("--worker-cores", required=True, type=int)
    parser.add_argument("--worker-memory", required=True)
    parser.add_argument("--worker-num", required=True, type=int)
    parser.add_argument("--worker-cores-overhead", required=False, type=int)
    parser.add_argument("--worker-timeout", required=False, default="10m")
    parser.add_argument("--pool", required=False)
    parser.add_argument("--tmpfs-limit", required=False, default="150G")
    parser.add_argument("--ssd-limit", required=False, default=None)
    parser.add_argument("--master-memory-limit", required=False, default="4G")
    parser.add_argument("--history-server-memory-limit", required=False, default="16G")
    parser.add_argument("--history-server-memory-overhead", required=False, default="4G")
    parser.add_argument("--history-server-cpu-limit", required=False, default=8, type=int)
    parser.add_argument("--operation-alias", required=False)
    parser.add_argument("--network-project", required=False)
    parser.add_argument("--params", required=False, action=ParseStructuredArgument, dest="params",
                        default=SparkDefaultArguments.get_params())
    parser.add_argument('--abort-existing', required=False, action='store_true', default=False)
    parser.add_argument("--spark-cluster-version", required=False)
    parser.add_argument("--worker-log-update-interval", required=False, default="10m")
    parser.add_argument("--shs-location", required=False)
    parser.add_argument("--preemption_mode", required=False, default="normal")

    default_enablers = SpytEnablers()
    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument('--enable-byop', dest='enable_byop', action='store_true')
    group.add_argument('--disable-byop', dest='enable_byop', action='store_false')
    parser.set_defaults(enable_byop=default_enablers.enable_byop)

    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument('--enable-profiling', dest='enable_profiling', action='store_true')
    group.add_argument('--disable-profiling', dest='enable_profiling', action='store_false')
    parser.set_defaults(enable_profiling=default_enablers.enable_profiling)

    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument('--enable-mtn', dest='enable_mtn', action='store_true')
    group.add_argument('--disable-mtn', dest='enable_mtn', action='store_false')
    parser.set_defaults(enable_mtn=default_enablers.enable_mtn)

    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument('--enable-advanced-event-log', dest='advanced_event_log', action='store_true')
    group.add_argument('--disable-advanced-event-log', dest='advanced_event_log', action='store_false')
    parser.set_defaults(advanced_event_log=False)

    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument('--enable-worker-log-transfer', dest='worker_log_transfer', action='store_true')
    group.add_argument('--disable-worker-log-transfer', dest='worker_log_transfer', action='store_false')
    parser.set_defaults(worker_log_transfer=False)

    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument('--enable-worker-log-json-mode', dest='worker_log_json_mode', action='store_true')
    group.add_argument('--disable-worker-log-json-mode', dest='worker_log_json_mode', action='store_false')
    parser.set_defaults(worker_log_json_mode=False)

    args, unknown_args = spark_utils.parse_args(parser)

    yt_client = YtClient(proxy=args.proxy, token=spark_utils.default_token())

    start_spark_cluster(worker_cores=args.worker_cores,
                        worker_memory=args.worker_memory,
                        worker_num=args.worker_num,
                        worker_cores_overhead=args.worker_cores_overhead,
                        worker_timeout=args.worker_timeout,
                        operation_alias=args.operation_alias,
                        discovery_path=args.discovery_path,
                        pool=args.pool or get_user_name(client=yt_client),
                        tmpfs_limit=args.tmpfs_limit,
                        ssd_limit=args.ssd_limit,
                        master_memory_limit=args.master_memory_limit,
                        history_server_memory_limit=args.history_server_memory_limit,
                        history_server_memory_overhead=args.history_server_memory_overhead,
                        history_server_cpu_limit=args.history_server_cpu_limit,
                        network_project=args.network_project,
                        tvm_id=spark_utils.default_tvm_id(),
                        tvm_secret=spark_utils.default_tvm_secret(),
                        abort_existing=args.abort_existing,
                        advanced_event_log=args.advanced_event_log,
                        worker_log_transfer=args.worker_log_transfer,
                        worker_log_json_mode=args.worker_log_json_mode,
                        worker_log_update_interval=args.worker_log_update_interval,
                        params=args.params,
                        shs_location=args.shs_location,
                        spark_cluster_version=args.spark_cluster_version,
                        enablers=SpytEnablers(
                            enable_byop=args.enable_byop,
                            enable_profiling=args.enable_profiling,
                            enable_mtn=args.enable_mtn
                        ),
                        client=yt_client,
                        preemption_mode=args.preemption_mode)


if __name__ == '__main__':
    main()
