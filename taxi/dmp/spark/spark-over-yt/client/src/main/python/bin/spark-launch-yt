#!/usr/bin/env python

import sys
import os
import subprocess

spark_home = os.environ.get("SPARK_HOME")
if spark_home is None:
    try:
        bin_dir = os.path.dirname(os.path.realpath(__file__))
        spark_home = subprocess.check_output("find_spark_home.py").strip().decode("utf-8")
    except:
        raise RuntimeError("Unable to find SPARK_HOME automatically from {}".format(os.path.realpath(__file__)))

sys.path.insert(0, os.path.join(spark_home, "bin", "python"))

from yt.wrapper import YtClient
from yt.wrapper.cli_helpers import ParseStructuredArgument
from yt.wrapper.http_helpers import get_user_name
from spyt.standalone import start_spark_cluster, find_spark_cluster, SparkDefaultArguments
from spyt import utils as spark_utils


def main():
    parser = spark_utils.get_default_arg_parser(description="Spark over YT")

    parser.add_argument("--worker-cores", required=True, type=int)
    parser.add_argument("--worker-memory", required=True)
    parser.add_argument("--worker-num", required=True, type=int)
    parser.add_argument("--worker-timeout", required=False, default="5m")
    parser.add_argument("--pool", required=False)
    parser.add_argument("--tmpfs-limit", required=False, default="150G")
    parser.add_argument("--master-memory-limit", required=False, default="2G")
    parser.add_argument("--history-server-memory-limit", required=False, default="8G")
    parser.add_argument("--operation-alias", required=False)
    parser.add_argument("--dynamic-config-path", required=False, default="//sys/spark/conf/releases/spark-launch-conf")
    parser.add_argument("--params", required=False, action=ParseStructuredArgument, dest="params",
                        default=SparkDefaultArguments.get_params())
    parser.add_argument("--spark-cluster-version", required=False)

    args, unknown_args = spark_utils.parse_args(parser)

    yt_client = YtClient(proxy=args.proxy, token=spark_utils.default_token())

    start_spark_cluster(worker_cores=args.worker_cores,
                        worker_memory=args.worker_memory,
                        worker_num=args.worker_num,
                        worker_timeout=args.worker_timeout,
                        operation_alias=args.operation_alias,
                        discovery_path=args.discovery_path,
                        pool=args.pool or get_user_name(client=yt_client),
                        tmpfs_limit=args.tmpfs_limit,
                        master_memory_limit=args.master_memory_limit,
                        history_server_memory_limit=args.history_server_memory_limit,
                        params=args.params,
                        spark_cluster_version=args.spark_cluster_version,
                        client=yt_client)


if __name__ == '__main__':
    main()
