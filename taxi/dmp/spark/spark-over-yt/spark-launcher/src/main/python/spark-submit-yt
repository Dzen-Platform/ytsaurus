#!/usr/bin/env python

import argparse
import os
from yt.wrapper.cypress_commands import list
import yt

parser = argparse.ArgumentParser(description="Spark Submit")
parser.add_argument("--id", required=True)
parser.add_argument("--working-dir", required=False)
parser.add_argument("--auto-auth", required=False, default=True, type=bool)
parser.add_argument("--proxy", required=False)

args, unknown_args = parser.parse_known_args()

id = args.id
user = os.getenv("USER")
proxy = args.proxy or os.getenv("YT_PROXY")
working_dir = args.working_dir or "//home/{0}/spark-tmp".format(user)
spark_home = os.getenv("SPARK_HOME")

yt.wrapper.config.config['proxy']['url'] = proxy
master = list("{0}/instances/{1}/rest".format(working_dir, id))[0]

spark_args = ["spark-submit", "--master", "spark://{0}".format(master)]
spark_env = os.environ.copy()

spark_args.append("--conf")
spark_args.append("spark.hadoop.yt.proxy={}".format(proxy))

spark_args.append("--conf")
spark_args.append("spark.master.rest.enabled=true")

if args.auto_auth:
    with open("{0}/.yt/token".format(os.getenv("HOME"))) as f:
        token = f.readline().strip()
    spark_args.append("--conf")
    spark_args.append("spark.hadoop.yt.user={}".format(user))
    spark_args.append("--conf")
    spark_args.append("spark.hadoop.yt.token={}".format(token))
    spark_env["SPARK_USER"] = user

os.execve("{0}/bin/spark-submit".format(spark_home), spark_args + unknown_args, spark_env)
