#!/usr/bin/env python

import yt.transfer_manager.server.traceback_helpers as traceback_helpers
from yt.transfer_manager.server.flask_helpers import process_gzip
from yt.transfer_manager.server.logger import TaskIdLogger
from yt.transfer_manager.server.pattern_matching import match_copy_pattern
from yt.transfer_manager.server.message_queue import MessageReader, MessageWriter
from yt.transfer_manager.server.yt_client import YtClientWithNotifications
from yt.transfer_manager.server.lock import CountedRLock

from yt.tools.logging_server import LogRecordSocketReceiver
from yt.tools.yamr import Yamr, YamrError
from yt.tools.hadoop import Airflow, Hdfs, Hive, HBase
from yt.tools.remote_copy_tools import Kiwi
from yt.tools.remote_copy_tools import \
    copy_yamr_to_yt_pull, \
    copy_yt_to_yamr_pull, \
    copy_yt_to_yamr_push, \
    copy_yt_to_kiwi, \
    copy_yt_to_yt, \
    copy_yt_to_yt_through_proxy, \
    copy_hive_to_yt, \
    copy_hadoop_to_hadoop_with_airflow

from yt.common import remove_file
from yt.wrapper.common import generate_uuid, get_value, update, parse_bool
from yt.wrapper.client import Yt
import yt.logger as logger
import yt.wrapper as yt
import yt.json as json
from yt.packages.expiringdict import ExpiringDict

from flask import Flask, request, jsonify, Response, make_response

import os
import sys
import time
import prctl
import signal
import socket
import logging
import logging.handlers
import tempfile
import thread
import argparse
import traceback
import subprocess
import pwd
from copy import deepcopy
from datetime import datetime, timedelta
from collections import defaultdict, deque
from subprocess32 import TimeoutExpired
from threading import RLock, Thread, current_thread
from multiprocessing import Process
from itertools import chain
import Queue

yt.config["read_retries"]["enable"] = True

class RequestFailed(yt.YtError):
    pass

class IncorrectTokenError(RequestFailed):
    pass

class SchedulingError(yt.YtError):
    pass

class SafeThread(Thread):
    def __init__(self, group=None, target=None, name=None, terminate=None, args=None, kwargs=None):
        self._parent_pid = os.getpid()

        if args is None:
            args = ()
        if kwargs is None:
            kwargs = {}

        def safe_run(*args, **kwargs):
            try:
                target(*args, **kwargs)
            except KeyboardInterrupt:
                print >>sys.stderr, "Interrupting main from child thread:", current_thread()
                thread.interrupt_main()
                time.sleep(0.5)
                os.kill(self._parent_pid, signal.SIGTERM)
            except:
                print >>sys.stderr, "Unknown exception"
                print >>sys.stderr, traceback.format_exc()
                logger.exception("Unknown exception")
                os.kill(self._parent_pid, signal.SIGINT)
                time.sleep(0.5)
                os.kill(self._parent_pid, signal.SIGTERM)

        super(SafeThread, self).__init__(group=group, target=safe_run, name=name, args=args, kwargs=kwargs)

SIGTERM_SENT = False
def sigterm_handler(signum, frame):
    global SIGTERM_SENT
    if not SIGTERM_SENT:
        print >>sys.stderr, "Handling SIGTERM, killing process group"
        SIGTERM_SENT = True
        os.killpg(0, signal.SIGINT)
        time.sleep(0.5)
        os.killpg(0, signal.SIGTERM)

    os._exit(1)

def log_yt_exception(logger, message=None):
    # Python2.7 has a bug in unpickling exception with non-trivial constructor.
    # To overcome it we convert exception to string before logging through socket handler.
    # https://bugs.python.org/issue1692335
    exc_type, exc_value, exc_traceback = sys.exc_info()
    exception_string = "\n".join(traceback.format_exception(exc_type, str(exc_value), exc_traceback))
    if message is not None:
        logger.error(message + "\n" + exception_string)
    else:
        logger.error(exception_string)

def filter_out_keys(dict, keys):
    result = deepcopy(dict)
    for key in keys:
        if key in result:
            del result[key]
    return result

def remove_unsigned(obj):
    def is_uint(num):
        return isinstance(num, long) and (num < -2 ** 63 or num >= 2 ** 63)

    if isinstance(obj, list):
        result = []
        for value in obj:
            new_value = remove_unsigned(value)
            if not is_uint(new_value):
                result.append(new_value)
        return result
    elif isinstance(obj, dict):
        result = {}
        for key, value in obj.iteritems():
            new_value = remove_unsigned(value)
            if not is_uint(new_value):
                result[key] = new_value
        return result
    else:
        return obj

def truncate_stderrs_attributes(error, limit):
    if hasattr(error, "attributes") and "stderrs" in error.attributes:
        if isinstance(error, yt.YtOperationFailedError):
            error.attributes["details"] = yt.format_operation_stderrs(error.attributes["stderrs"])[:limit]
        elif isinstance(error, YamrError):
            error.attributes["details"] = error.attributes["stderrs"][:limit]
        del error.attributes["stderrs"]
    if hasattr(error, "inner_errors"):
        for inner_error in error.inner_errors:
            truncate_stderrs_attributes(inner_error, limit)

def now_str():
    return str(datetime.utcnow().isoformat() + "Z")

class Task(object):
    # NB: destination table is missing if we copy to kiwi
    def __init__(self, source_cluster, source_table, destination_cluster, creation_time, id, state, user,
                 destination_table=None, source_cluster_token=None, token=None, destination_cluster_token=None, mr_user=None, error=None,
                 start_time=None, finish_time=None, copy_method=None, skip_if_destination_exists=None, progress=None, backend_tag=None, kiwi_user=None, kwworm_options=None, pool=None, meta=None,
                 destination_compression_codec=None, destination_erasure_codec=None, destination_force_sort=None, copy_spec=None, postprocess_spec=None,
                 job_timeout=None, lease_timeout=None, queue_name=None):
        self.source_cluster = source_cluster
        self.source_table = source_table
        self.source_cluster_token = get_value(source_cluster_token, token)
        self.destination_cluster = destination_cluster
        self.destination_table = destination_table
        self.destination_cluster_token = get_value(destination_cluster_token, token)

        self.creation_time = creation_time
        self.start_time = start_time
        self.finish_time = finish_time
        self.state = state
        self.id = id
        self.user = user
        self.mr_user = mr_user
        self.error = error
        self.token = token
        self.copy_method = copy_method
        self.skip_if_destination_exists = skip_if_destination_exists
        self.progress = progress

        self.backend_tag = backend_tag
        self.pool = pool
        self.kiwi_user = kiwi_user
        self.kwworm_options = kwworm_options
        self.copy_spec = copy_spec
        self.postprocess_spec = postprocess_spec
        # NB: supported only for yamr_to_yt_pull
        self.job_timeout = job_timeout

        self.destination_compression_codec = destination_compression_codec
        self.destination_erasure_codec = destination_erasure_codec
        self.destination_force_sort = destination_force_sort

        self.lease_timeout = lease_timeout
        self.queue_name = queue_name

        # Special field to store meta-information for web-interface
        self.meta = meta

        self._last_ping_time = None


    def get_queue_id(self):
        return self.queue_name, self.source_cluster, self.destination_cluster

    def get_source_client(self, clusters):
        if self.source_cluster not in clusters:
            raise yt.YtError("Unknown cluster " + self.source_cluster)
        client = deepcopy(clusters[self.source_cluster])
        if client._type == "yt":
            client.config["token"] = self.source_cluster_token
        if client._type == "yamr" and self.mr_user is not None:
            client.mr_user = self.mr_user
        return client

    def get_destination_client(self, clusters):
        if self.destination_cluster not in clusters:
            raise yt.YtError("Unknown cluster " + self.destination_cluster)
        client = deepcopy(clusters[self.destination_cluster])
        if client._type == "yt":
            client.config["token"] = self.destination_cluster_token
        if client._type == "yamr" and self.mr_user is not None:
            client.mr_user = self.mr_user
        return client

    def dict(self, hide_token=False, fields=None):
        result = self.__dict__.copy()
        if hide_token:
            for key in ["token", "source_cluster_token", "destination_cluster_token"]:
                del result[key]
        for key in result.keys():
            if result[key] is None or (fields is not None and key not in fields) or key.startswith("_"):
                del result[key]
        return result

    def copy(self):
        return Task(**dict((key, value) for key, value in self.__dict__.iteritems() if not key.startswith("_")))

class ClustersConfiguration(object):
    def __init__(self, clusters, availability_graph, kiwi_transmitter, airflow_client):
        self.clusters = clusters
        self.availability_graph = availability_graph
        self.kiwi_transmitter = kiwi_transmitter
        self.airflow_client = airflow_client

class Application(object):
    ERROR_BUFFER_SIZE = 2 ** 16

    def __init__(self, config_path):
        self._start_time = time.time()
        self._daemon = Flask(__name__)

        self._config_path = config_path
        self._config = json.load(open(config_path))
        self._last_successfully_loaded_config = deepcopy(self._config)
        self._mutex = CountedRLock()
        self._cypress_mutexes = defaultdict(CountedRLock)
        self._yt = Yt(**self._config["yt_backend_options"])

        self._profiling_mutex = RLock()
        self._profiling = {"execution_time": defaultdict(deque)}

        self._configure_logging(self._config.get("logging", {}))

        self._clusters_config_mutex = RLock()
        self._clusters_configuration = self._get_clusters_configuration_from_config(self._config)

        self._path = self._config["path"]
        self._logging_port = self._config["port"] + 1

        self._yandex_yt_python_tools_version = self._get_yandex_yt_python_tools_version()

    def init_logger(self):
        # Replace current logger with socket logger.
        logger.LOGGER.handlers = [logging.handlers.SocketHandler("localhost", self._logging_port)]

    def start_processes(self):
        self._sleep_step = 0.5

        # Run logging process. It inherits current logging configuration.
        self._logging_process = Process(target=self._run_logging_server, name="LoggingThread")
        self._logging_process.daemon = True
        self._logging_process.start()

        self.init_logger()

        # Prepare auth node if it is missing
        self._auth_path = os.path.join(self._path, "auth")
        self._yt.create("map_node", self._auth_path, ignore_existing=True)

        # Run lock thread
        self._terminating = False
        self._lock_acquired = False
        self._lock_path = os.path.join(self._path, "lock")
        self._lock_timeout = 10.0
        self._yt.create("map_node", self._lock_path, ignore_existing=True)
        self._lock_thread = SafeThread(target=self._take_lock, name="LockThread")
        self._lock_thread.daemon = True
        self._lock_thread.start()

        # Run execution thread
        self._task_processes = {}
        self._execution_thread = SafeThread(target=self._schedule_tasks, name="SchedulingThread")
        self._execution_thread.daemon = True
        self._execution_thread.start()

        # Run config reloading thread.
        self._clusters_config_reloading_thread = SafeThread(target=self._reload_clusters_config,
                                                            args=(self._config["clusters_config_reload_timeout"],),
                                                            name="ClustersConfigReloadingThread")
        self._clusters_config_reloading_thread.daemon = True
        self._clusters_config_reloading_thread.start()

        # Initialize mutating requests cache.
        self._mutating_requests_cache = ExpiringDict(
            self._config["mutating_requests_cache_size"],
            self._config["mutating_requests_cache_expiration_timeout"] / 1000.0)

        # Add rules
        self._add_rule("/", 'main', methods=["GET"], depends_on_lock=False)
        self._add_rule("/tasks/", 'get_tasks', methods=["GET"])
        self._add_rule("/tasks/", 'add', methods=["POST"], is_mutating=True)
        self._add_rule("/tasks/<id>/", 'get_task', methods=["GET"])
        self._add_rule("/tasks/<id>/", 'delete_task', methods=["DELETE"], is_mutating=True)
        self._add_rule("/tasks/<id>/abort/", 'abort', methods=["POST"], is_mutating=True)
        self._add_rule("/tasks/<id>/restart/", 'restart', methods=["POST"], is_mutating=True)
        self._add_rule("/tasks/<id>/ping/", 'ping_task', methods=["POST"])
        self._add_rule("/config/", 'config', methods=["GET"])
        self._add_rule("/ping/", 'ping', methods=["GET"])
        self._add_rule("/match/", 'match', methods=["POST"])
        self._add_rule("/profiling/", 'profiling', methods=["GET"])

    def terminate(self):
        self._terminating = True
        self._lock_thread.join(self._sleep_step)
        self._execution_thread.join(self._sleep_step)

    def _add_rule(self, rule, endpoint, methods, depends_on_lock=True, is_mutating=False):
        methods.append("OPTIONS")

        middlewares = [(self._profile_time, (endpoint, )),
                       (self._process_lock, (depends_on_lock, )),
                       (process_gzip, ()),
                       (self._process_cors, (methods, )),
                       (self._process_exception, ())]
        if is_mutating:
            middlewares.insert(1, (self._process_mutating_request, ()))

        endpoint_method = Application.__dict__[endpoint]
        for middleware, middleware_args in reversed(middlewares):
            endpoint_method = middleware(endpoint_method, *middleware_args)

        self._daemon.add_url_rule(rule, endpoint, endpoint_method, methods=methods)

    def _process_lock(self, func, depends_on_lock):
        def decorator(*args, **kwargs):
            if depends_on_lock and not self._lock_acquired:
                return "Cannot take lock", 500
            return func(*args, **kwargs)
        return decorator

    def _process_cors(self, func, methods):
        def decorator(*args, **kwargs):
            if request.method == "OPTIONS":
                rsp = self._daemon.make_default_options_response()
                rsp.headers["Access-Control-Allow-Origin"] = "*"
                rsp.headers["Access-Control-Allow-Methods"] = ", ".join(methods)
                rsp.headers["Access-Control-Allow-Headers"] = ", ".join(["Authorization", "Origin", "Content-Type", "Accept"])
                rsp.headers["Access-Control-Max-Age"] = 3600
                return rsp
            else:
                rsp = make_response(func(self, *args, **kwargs))
                rsp.headers["Access-Control-Allow-Origin"] = "*"
                return rsp

        return decorator

    def _process_exception(self, func):
        def decorator(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except RequestFailed as error:
                log_yt_exception(logger)
                return json.dumps(error.simplify()), 404
            except Exception as error:
                logger.exception("Unknown error:")
                return json.dumps({"code": 1, "message": "Unknown error: " + str(error)}), 404

        return decorator

    def _profile_time(self, func, method_name):
        def decorator(*args, **kwargs):
            now = time.time()
            result = func(*args, **kwargs)
            elapsed = time.time() - now

            with self._profiling_mutex:
                self._profiling["execution_time"][method_name].append((datetime.now().isoformat(), elapsed))
                if len(self._profiling["execution_time"][method_name]) > self._config["profiling"]["queue_limit"]:
                    self._profiling["execution_time"][method_name].popleft()

            return result

        return decorator

    def _process_mutating_request(self, func):
        def decorator(*args, **kwargs):
            params = json.loads_as_bytes(request.headers.get("X-TM-Parameters", "{}"))
            mutation_id = params.get("mutation_id")
            retry = parse_bool(params.get("retry", "false"))

            if retry and time.time() - self._start_time < self._mutating_requests_cache.max_age:
                raise RequestFailed("Transfer Manager just started and cannot process retry requests")

            if retry and not mutation_id:
                raise RequestFailed('Cannot process retry request without specified "mutation_id" header')

            if retry:
                processed_request = self._mutating_requests_cache.get(mutation_id)
                if processed_request is not None:
                    return processed_request
                else:
                    raise RequestFailed("Cached response for request with mutation id {0} not found"
                                        .format(mutation_id))
            else:
               if mutation_id in self._mutating_requests_cache:
                    raise RequestFailed('Duplicate request with mutation id {0} is not marked as "retry"'\
                                        .format(mutation_id))
               else:
                   response = func(*args, **kwargs)
                   if mutation_id is not None:
                       self._mutating_requests_cache[mutation_id] = response
                   return response

        # XXX(asaitgalin): Decorator is wrapped with process_exception for correct handling RequestFailed
        # (client expects to receive json, so process_exception will dump RequestFailed to json)
        return self._process_exception(decorator)

    def _take_lock(self):
        yt_client = deepcopy(self._yt)
        with yt_client.Transaction():
            while True:
                try:
                    yt_client.lock(self._lock_path)
                    break
                except yt.YtError as err:
                    if err.is_concurrent_transaction_lock_conflict():
                        logger.info("Failed to take lock")
                        time.sleep(self._lock_timeout)
                        continue
                    log_yt_exception(logger)
                    return

            logger.info("Lock acquired")

            # Loading tasks from cypress
            self._load_tasks(os.path.join(self._path, "tasks"))

            self._lock_acquired = True

            # Set attribute outside of transaction
            self._yt.set_attribute(self._path, "address", socket.getfqdn())

            # Sleep infinitely long
            while True:
                if self._terminating:
                    return
                time.sleep(self._sleep_step)

    def _get_clusters_configuration(self):
        with self._clusters_config_mutex:
            return deepcopy(self._clusters_configuration)

    def _get_clusters_configuration_from_config(self, config):
        config = deepcopy(config)
        clusters = {}
        for name, cluster_description in config["clusters"].iteritems():
            type = cluster_description["type"]
            options = cluster_description.get("options", {})

            if type == "yt":
                if "config" not in options:
                    options["config"] = {}
                options["config"] = update(config["yt_client_config"], options["config"])
                clusters[name] = YtClientWithNotifications(**options)
                clusters[name]._pools = cluster_description.get("pools", {})
                clusters[name]._version = cluster_description.get("version", 0)
            elif type == "yamr":
                clusters[name] = Yamr(**options)
            elif type == "kiwi":
                clusters[name] = Kiwi(**options)
            elif type == "hive":
                clusters[name] = Hive(**options)
            elif type == "hdfs":
                clusters[name] = Hdfs(**options)
            elif type == "hbase":
                clusters[name] = HBase(**options)
            else:
                raise yt.YtError("Incorrect cluster type " + options["type"])

            clusters[name]._name = name
            clusters[name]._type = type
            clusters[name]._parameters = filter_out_keys(cluster_description, ["type", "options"])

        availability_graph = config["availability_graph"]

        for name in availability_graph:
            edges = {}
            for neighbour in availability_graph[name]:
                if isinstance(neighbour, dict):
                    edges[neighbour["name"]] = neighbour.get("options", {})
                else:
                    edges[neighbour] = {}
            availability_graph[name] = edges

        for name in availability_graph:
            if name not in clusters:
                raise yt.YtError("Incorrect availability graph, cluster {} is missing".format(name))
            for neighbour in availability_graph[name]:
                if neighbour not in clusters:
                    raise yt.YtError("Incorrect availability graph, cluster {} is missing".format(neighbour))

        kiwi_transmitter = None
        if "kiwi_transmitter" in config:
            name = config["kiwi_transmitter"]
            if name not in clusters:
                raise yt.YtError("Incorrect kiwi transmitter, cluster {} is missing".format(name))
            kiwi_transmitter = clusters[name]
            if kiwi_transmitter._type != "yt":
                raise yt.YtError("Kiwi transmitter must be YT cluster")

        airflow_client = None
        if "hadoop_transmitter" in config:
            hadoop_transmitter = config["hadoop_transmitter"]
            if hadoop_transmitter["type"] != "airflow":
                raise yt.YtError("Hadoop transmitter must be airflow client")
            airflow_client = Airflow(**hadoop_transmitter["options"])
            airflow_client._name = hadoop_transmitter["name"]

        return ClustersConfiguration(clusters, availability_graph, kiwi_transmitter, airflow_client)

    def _reload_clusters_config(self, reload_timeout):
        while True:
            if not self._lock_acquired:
                time.sleep(self._lock_timeout)
                continue

            time.sleep(reload_timeout)

            try:
                config = json.load(open(self._config_path))
                with self._clusters_config_mutex:
                    if self._last_successfully_loaded_config == config:
                        logger.info("Config is not changed, reloading skipped")
                        continue

                configuration = self._get_clusters_configuration_from_config(config)
                with self._clusters_config_mutex:
                    self._clusters_configuration = configuration
                    self._last_successfully_loaded_config = config

                logger.info("Clusters configuration was successfully reloaded")
            except:
                logger.exception("Failed to update clusters configuration")

    def _configure_logging(self, logging_node):
        level = logging.__dict__[logging_node.get("level", "INFO")]

        if "filename" in logging_node:
            handler = logging.handlers.WatchedFileHandler(logging_node["filename"])
        else:
            handler = logging.StreamHandler()

        new_logger = logging.getLogger("Transfer manager")
        new_logger.propagate = False
        new_logger.setLevel(level)
        new_logger.handlers = [handler]
        new_logger.handlers[0].setFormatter(logger.BASIC_FORMATTER)
        logger.LOGGER = new_logger

        logging.getLogger("werkzeug").setLevel(level)
        logging.getLogger("werkzeug").addHandler(handler)

    def _run_logging_server(self):
        prctl.set_pdeathsig(signal.SIGINT)
        socket_reciever = LogRecordSocketReceiver(lambda record: logger.LOGGER.handle(record),
                                                  port=self._logging_port)
        socket_reciever.serve_until_stopped()

    def _load_tasks(self, tasks_path):
        logger.info("Loading tasks from cypress")

        self._tasks_path = tasks_path
        if not self._yt.exists(self._tasks_path):
            self._yt.create("map_node", self._tasks_path)

        # From id to task description
        self._tasks = {}

        # From ... to task ids
        self._running_tasks_queues = defaultdict(lambda: [])
        self._runned_during_heartbeat = set()
        self._last_scheduled_time = defaultdict(lambda: datetime(year=1970, month=1, day=1))

        # List of tasks sorted by creation time
        self._pending_tasks = []

        # Number of pending tasks per user
        self._pending_tasks_per_user = defaultdict(lambda: 0)

        for id, options in self._yt.get(tasks_path).iteritems():
            try:
                task = Task(**options)
                task._last_ping_time = time.time()
            except Exception as err:
                logger.warning("Failed to load task %s: %s", id, str(err))
                continue

            try:
                active_client, passive_client = self._get_active_and_passive_clients(
                    task, self._get_clusters_configuration())
            except yt.YtError as err:
                logger.warning("Failed to load task %s: %s", id, str(err))
                continue

            if active_client._type == "yt" and task.pool is None:
                logger.warning("Failed load task %s: task has no pool", id)
                continue

            self._tasks[id] = task
            if task.state == "running":
                self._change_task_state(id, "pending")
            if task.state == "pending":
                self._append_pending_task(task)

        self._pending_tasks.sort(key=lambda id: self._tasks[id].creation_time)

        # Abort tasks operations (TM can fail but operations will go on)
        self._abort_running_operations()

        logger.info("Tasks load")

    def _append_pending_task(self, task):
        self._pending_tasks.append(task.id)
        self._pending_tasks_per_user[task.user] += 1

    def _change_task_state(self, id, new_state):
        with self._mutex:
            old_state = self._tasks[id].state
            self._tasks[id].state = new_state
            self._dump_task(id)
            logger.info("Task %s change state: %s -> %s", id, old_state, new_state)

    def _dump_task(self, id):
        with self._mutex:
            path = os.path.join(self._tasks_path, id)
            value = remove_unsigned(self._tasks[id].dict())
            os.path.join(self._tasks_path, id)
            with self._mutex.discharge():
                client = Yt(**self._config["yt_backend_options"])
                with self._cypress_mutexes[path]:
                    client.set(path, value)
            if not self._cypress_mutexes[path].is_acquired():
                del self._cypress_mutexes[path]

    def _create_pool(self, yt_client, destination_cluster_name):
        pool_name = "transfer_" + destination_cluster_name
        pool_path = "//sys/pools/transfer_manager/" + pool_name
        yt_client = deepcopy(yt_client)
        yt_client.config["token"] = self._yt.config["token"]
        try:
            if not yt_client.exists(pool_path):
                yt_client.create("map_node", pool_path, recursive=True, ignore_existing=True)
                yt_client.set(pool_path + "/@resource_limits", {"user_slots": 200})
                yt_client.set(pool_path + "/@mode", "fifo")
            while not yt_client.exists("//sys/scheduler/orchid/scheduler/pools/" + pool_name):
                time.sleep(0.5)
        except yt.YtError as err:
            raise yt.YtError("Transfer manager failed to create pool path: %s" % pool_path, inner_errors=[err])
        return pool_name

    # Return pair: active client, that run copy operation and passive client.
    def _get_active_and_passive_clients(self, task, clusters_configuration):
        source_client = task.get_source_client(clusters_configuration.clusters)
        destination_client = task.get_destination_client(clusters_configuration.clusters)

        if destination_client._type == "yamr":
            if source_client._type == "yt" and task.copy_method == "push":
                return source_client, destination_client
            return destination_client, source_client
        elif destination_client._type == "yt":
            return destination_client, source_client
        elif destination_client._type == "kiwi":
            return clusters_configuration.kiwi_transmitter, source_client
        elif destination_client._type in ["hive", "hdfs", "hbase"]:
            return source_client, destination_client
        else:
            raise yt.YtError("Unknown destination client type: %r", destination_client._type)

    def _initialize_pool(self, task, clusters_configuration):
        if task.pool is not None:
            return

        active_client, passive_client = self._get_active_and_passive_clients(task, clusters_configuration)
        if active_client._type != "yt":
            return

        name = passive_client._name
        if name in active_client._pools:
            pool = active_client._pools[name]
        else:
            pool = self._create_pool(active_client, name)
        task.pool = pool

    def _get_token(self, authorization_header):
        words = authorization_header.split()
        if len(words) != 2 or words[0].lower() != "oauth":
            return None
        return words[1]

    def _get_token_and_user(self, headers_):
        headers = {}
        for key, value in headers_.iteritems():
            headers[key] = value

        # Some headers should not be passed.
        # NB: Passing Host header causes infinite redirect on locke
        for name in ["Host", "Content-Length"]:
            if name in headers:
                del headers[name]

        token = self._get_token(headers.get("Authorization", ""))
        if "X-Forwarded-For" in headers:
            #headers["X-Forwarded-For"] += ", " + request.remote_addr
            # NB: it is not yet supported by proxy.
            pass
        else:
            headers["X-Forwarded-For"] = request.remote_addr
        if token == "undefined":
            user = "guest"
            token = ""
        else:
            user = self._yt.get_user_name(token, headers=headers)
            if not user:
                raise IncorrectTokenError("Authorization token is incorrect: " + token)
        return token, user

    @staticmethod
    def _get_yandex_yt_python_tools_version():
        if os.path.isfile("/etc/yandex_yt_python_tools/version"):
            return open("/etc/yandex_yt_python_tools/version").read().strip()
        return None

    @staticmethod
    def _has_write_permission_on_yt(client, user, dir):
        while not client.exists(dir):
            dir = dir.rsplit("/", 1)[0]
        return client.check_permission(user, "write", dir)["action"] == "allow"

    def _check_permission(self, id, headers, action_name):
        _, user = self._get_token_and_user(headers)
        if self._tasks[id].user != user and \
           self._yt.check_permission(user, "administer", self._auth_path)["action"] != "allow":
            raise RequestFailed("{0} task is not permitted.".format(action_name))

    def _set_defaults(self, task, clusters_configuration):
        source_client = task.get_source_client(clusters_configuration.clusters)
        destination_client = task.get_destination_client(clusters_configuration.clusters)
        if task.mr_user is None and (source_client._type == "yamr" or destination_client._type == "yamr"):
            task.mr_user = self._config.get("default_mr_user")
        if task.kiwi_user is None and destination_client._type == "kiwi":
            task.kiwi_user = self._config.get("default_kiwi_user")
        if task.copy_method is None and source_client._type == "yt" and destination_client._type == "yamr":
            task.copy_method = "pull"
        if task.copy_method is None and source_client._type == "yt" and destination_client._type == "yt":
            task.copy_method = "proxy" if source_client._version != destination_client._version else "native"

    def _precheck(self, task, clusters_configuration, ignore_timeout=False, yamr_timeout=None, custom_logger=None):
        if custom_logger is None:
            custom_logger = logger

        # TODO(ignat): add timeout for yt
        custom_logger.info("Starting precheck")
        source_client = task.get_source_client(clusters_configuration.clusters)
        destination_client = task.get_destination_client(clusters_configuration.clusters)

        if task.copy_method not in [None, "pull", "push", "proxy", "native"]:
            raise yt.YtError("Incorrect copy method: " + str(task.copy_method))

        if task.source_cluster not in clusters_configuration.availability_graph or \
           task.destination_cluster not in clusters_configuration.availability_graph[task.source_cluster]:
            raise yt.YtError("Cluster {} not available from {}".format(task.destination_cluster, task.source_cluster))

        if task.destination_table is None and destination_client._type != "kiwi":
            raise yt.YtError("Destination table should be specified for copying to {0}".format(destination_client._type))

        try:
            if yamr_timeout is not None:
                if source_client._type == "yamr":
                    source_client._light_command_timeout = yamr_timeout
                if destination_client._type == "yamr":
                    destination_client._light_command_timeout = yamr_timeout

            if (source_client._type == "yamr" and source_client.is_empty(task.source_table)) or \
               (source_client._type == "yt" and not source_client.exists(task.source_table)):
                raise yt.YtError("Source table {} is empty or does not exist".format(task.source_table))

            if source_client._type == "yt" and destination_client._type == "yamr":
                if source_client.row_count(task.source_table) > 0:
                    path = yt.TablePath(task.source_table, end_index=1, simplify=False, client=source_client)
                    keys = list(source_client.read_table(path, format=yt.JsonFormat(), raw=False).next())
                    if set(keys + ["subkey"]) != set(["key", "subkey", "value"]):
                        raise yt.YtError("Keys in the source table must be a subset of ('key', 'subkey', 'value')")

            if source_client._type == "yt" and destination_client._type == "yt":
                if task.copy_method not in ["proxy", "native"]:
                    raise yt.YtError("Incorrect copy method {0} for YT to YT copy task".format(task.copy_method))
                if task.copy_method == "native" and source_client._version != destination_client._version:
                    raise yt.YtError("Native copy method should not be specified for clusters with "
                                     "different YT versions")

            if destination_client._type == "yt":
                destination_dir = os.path.dirname(task.destination_table)
                # NB: copy operations usually create directories if it do not exits.
                #if not destination_client.exists(destination_dir):
                #    raise yt.YtError("Destination directory {} should exist".format(destination_dir))
                destination_user = destination_client.get_user_name(task.destination_cluster_token)
                if destination_user is None or not self._has_write_permission_on_yt(destination_client, destination_user, destination_dir):
                    raise yt.YtError("There is no permission to write to {}. Please log in.".format(task.destination_table))

            if destination_client._type == "kiwi" and clusters_configuration.kiwi_transmitter is None:
                raise yt.YtError("Transimission cluster for transfer to kiwi is not configured")

            if source_client._type == "hive" and "." not in task.source_table:
                raise yt.YtError("Incorrect source table for hive, it must have format {database_name}.{table_name}")

            if (source_client._type in ["hive", "hdfs", "hbase"]) and clusters_configuration.airflow_client is None:
                raise yt.YtError("Airflow client should be configured for transfer from {0} to {1}"
                                 .format(source_client._type, destination_client._type))

        except TimeoutExpired:
            custom_logger.info("Precheck timed out")
            if not ignore_timeout:
                raise
            return

        custom_logger.info("Precheck completed")

    def _can_run(self, task):
        if task.get_queue_id() in self._runned_during_heartbeat:
            return False
        if len(self._running_tasks_queues[task.get_queue_id()]) >= self._config["running_tasks_limit_per_direction"]:
            return False
        if datetime.now() - self._last_scheduled_time[task.get_queue_id()] < timedelta(seconds=self._config["scheduling_backoff"]):
            return False

        client, _ = self._get_active_and_passive_clients(task, self._get_clusters_configuration())
        if client._type == "yt":
            if task.pool is None:
                raise SchedulingError("Task has no pool")
            #pool_path = "//sys/scheduler/orchid/scheduler/pools/" + task.pool

            try:
                #if not client.exists(pool_path):
                #    raise SchedulingError("Pool %s does not exist" % task.pool)

                # TODO(ignat): uncomment after fix of resource_limits in 0.17 (YT-2222).
                #pool_info = client.get("//sys/scheduler/orchid/scheduler/pools/" + task.pool)
                ## This usage ratio differ from scheduler usage ratio, because it is calculated due to resource limits of pool.
                #usage_ratio = 0.0
                #for resource, usage in pool_info["resource_usage"].iteritems():
                #    limit = pool_info["resource_limits"][resource]
                #    if limit == 0:
                #        usage_ratio = 1.0
                #    else:
                #        usage_ratio = max(usage_ratio, float(usage) / limit)
                #if usage_ratio > 0.95:
                #    logger.warning("Pool %s on cluster %s is almost full, can not schedule task %s", task.pool, client.config["proxy"]["url"], task.id)
                #    result = False
                #else:
                #    result = True

                #return result
                return True
            except yt.YtError as error:
                new_error = SchedulingError("Cannot check pool")
                new_error.inner_errors = [error]
                raise new_error

        if client._type == "yamr":
            def is_transfer_manager_copy_operation(op):
                return "read_from_yt" in op.get("additionalInfo", {}).get("application", "") \
                    and op.get("systemUser") == pwd.getpwuid(os.getuid())[0]
            transfer_manager_operations = filter(is_transfer_manager_copy_operation, client.get_operations())
            job_count = sum(op.get("jobsInfo", {}).get("States", {}).get("Jobs", {}).get("inProgress", 0)
                            for op in transfer_manager_operations)
            if job_count > self._config["running_jobs_limit_per_direction"]:
                return False

        return True

    def _renew_lease(self, task_id):
        with self._mutex:
            # NB: Fields of Task must be serializable.
            self._tasks[task_id]._last_ping_time = time.time()

    def _force_abort(self, process):
        process.aborted = True
        if process.poll() is None:
            try:
                os.kill(process.pid, signal.SIGTERM)
            except OSError:
                pass

    # NB: return generator for waiting actual task abort.
    def _abort_task(self, id):
        process = None
        with self._mutex:
            if self._tasks[id].state in ["aborting", "aborted", "completed", "failed"]:
                return

            logger.info("Aborting task %s", id)

            task_was_executed = id in self._task_processes
            task_is_aborted = False

            if task_was_executed:
                process, _ = self._task_processes[id]
                process.aborting = True
                process.aborting_start_time = datetime.now()
                logger.info("Killing process with pid %d", process.pid)
                try:
                    os.kill(process.pid, signal.SIGINT)
                except OSError:
                    pass
                if process.poll() is not None:
                    process.aborted = True
                    task_is_aborted = True

            if id in self._pending_tasks:
                self._pending_tasks_per_user[self._tasks[id].user] -= 1
                self._pending_tasks.remove(id)

            if self._tasks[id].state not in ["aborted", "completed", "failed"]:
                new_state = "aborted"
                if task_was_executed and not task_is_aborted:
                    new_state = "aborting"
                self._change_task_state(id, new_state)

        if not task_was_executed:
            return

        def wait():
            while True:
                with self._mutex:
                    if process.aborted:
                        return
                yield

        return wait()

    def _schedule_tasks(self):
        while True:
            if self._terminating:
                return

            if not self._lock_acquired:
                time.sleep(self._lock_timeout)
                continue

            logger.info("Starting new scheduling round")

            with self._mutex:
                logger.info("Progress: %d running, %d pending tasks found", len(self._task_processes), len(self._pending_tasks))

                now = time.time()
                for id in chain(self._pending_tasks, self._task_processes):
                    task = self._tasks[id]
                    if task.lease_timeout is not None and (now - task._last_ping_time > task.lease_timeout):
                        self._abort_task(id)

                for id, (process, message_queue) in self._task_processes.items():
                    logger.info("Task %s %s", id, self._tasks[id].state)
                    error = None
                    completed = False
                    complete_type = None

                    process_is_alive = process.poll() is None

                    while not message_queue.empty():
                        message = None
                        try:
                            message = message_queue.get_nowait()
                        except Queue.Empty:
                            break

                        if message["type"] == "error":
                            error = message["error"]
                        elif message["type"] == "operation_started":
                            self._tasks[id].progress["operations"].append(message["operation"])
                            self._dump_task(id)
                        elif message["type"] in ["completed", "skipped"]:
                            completed = True
                            complete_type = message["type"]
                        else:
                            assert False, "Incorrect message type: " + message["type"]

                    if process.aborting and not process.aborted:
                        if (datetime.now() - process.aborting_start_time).total_seconds() > self._sleep_step:
                            self._force_abort(process)
                            self._change_task_state(id, "aborted")
                        else:
                            continue

                    if not process_is_alive and not (process.aborting or process.aborted or error is not None or completed):
                        message = "Process died silently"
                        logger.warning(message)
                        error = {"message": message, "code": 1}

                    if process.aborted or error is not None or completed:
                        self._tasks[id].finish_time = now_str()
                        if process.aborted:
                            pass
                        elif error is not None:
                            self._tasks[id].error = error
                            self._change_task_state(id, "failed")
                        elif completed:
                            if process_is_alive:
                                logger.warning("Task completed, but process still alive!")
                            self._change_task_state(id, complete_type)

                        self._dump_task(id)
                        self._running_tasks_queues[self._tasks[id].get_queue_id()].remove(id)
                        remove_file(process.temporary_config, force=True)
                        del self._task_processes[id]

                runned_tasks = 0
                for id in self._pending_tasks:
                    try:
                        if not self._can_run(self._tasks[id]):
                            logger.info("Task %s pending", id)
                            continue
                    except SchedulingError as error:
                        self._tasks[id].error = error.simplify()
                        self._change_task_state(id, "failed")
                        continue

                    queue_id = self._tasks[id].get_queue_id()
                    self._last_scheduled_time[queue_id] = datetime.now()
                    self._running_tasks_queues[queue_id].append(id)
                    self._runned_during_heartbeat.add(queue_id)
                    self._pending_tasks_per_user[self._tasks[id].user] -= 1

                    self._tasks[id].start_time = now_str()
                    self._tasks[id].progress = {"operations": []}
                    self._change_task_state(id, "running")

                    with tempfile.NamedTemporaryFile(delete=False, dir=self._config.get("default_tmp_dir")) as fout:
                        temporary_config = fout.name
                        json.dump(self._get_config(), fout)

                    task_process = subprocess.Popen(["transfer-manager-server", "--execute-task", "--config", temporary_config],
                                                    stdout=subprocess.PIPE, stdin=subprocess.PIPE,
                                                    preexec_fn=lambda: prctl.set_pdeathsig(signal.SIGINT))
                    task_process.stdin.write(json.dumps(self._tasks[id].dict()))
                    task_process.stdin.close()
                    task_process.aborting = False
                    task_process.aborted = False
                    task_process.temporary_config = temporary_config
                    queue = MessageReader(task_process.stdout)
                    self._task_processes[id] = (task_process, queue)

                    runned_tasks += 1
                    if runned_tasks >= self._config["max_tasks_to_run_per_scheduling_round"]:
                        break

                self._pending_tasks = filter(lambda id: self._tasks[id].state == "pending", self._pending_tasks)
                self._runned_during_heartbeat.clear()

            logger.info("Scheduling round takes %.2lf seconds")

            time.sleep(self._sleep_step)

    def execute_task(self, task, message_queue):
        logger.LOGGER = TaskIdLogger(task.id)

        logger.info("Start executing task (pid %s)", os.getpid())
        try:
            clusters_configuration = self._get_clusters_configuration()
            self._precheck(task, clusters_configuration)

            title = "Supervised by transfer task " + task.id

            common_spec = {
                "title": title,
                "transfer_manager": {
                    "task_id": task.id,
                    "backend_tag": task.backend_tag
                }
            }

            copy_spec = update(get_value(task.copy_spec, {}), common_spec)
            copy_spec["pool"] = task.pool

            postprocess_spec = update(get_value(task.postprocess_spec, {}), common_spec)

            source_client = task.get_source_client(clusters_configuration.clusters)
            source_client.message_queue = message_queue

            destination_client = task.get_destination_client(clusters_configuration.clusters)
            destination_client.message_queue = message_queue

            clusters_configuration.kiwi_transmitter.message_queue = message_queue

            parameters = clusters_configuration.availability_graph[task.source_cluster][task.destination_cluster]

            # Calculate fastbone
            fastbone = source_client._parameters.get("fastbone", False) and destination_client._parameters.get("fastbone", False)
            fastbone = parameters.get("fastbone", fastbone)

            skipped = False
            if task.skip_if_destination_exists and (
                    (destination_client._type == "yamr" and not destination_client.is_empty(task.destination_table)) or \
                    (destination_client._type == "yt" and destination_client.exists(task.destination_table))):
                logger.info("Copying skipped since 'skip_if_destination_exists' option specified and destination table exists")
                skipped = True
            elif source_client._type == "yt" and destination_client._type == "yt":
                logger.info("Running YT -> YT remote copy operation")
                if task.copy_method == "proxy":
                    copy_yt_to_yt_through_proxy(
                        source_client,
                        destination_client,
                        task.source_table,
                        task.destination_table,
                        fastbone=fastbone,
                        copy_spec_template=copy_spec,
                        postprocess_spec_template=postprocess_spec,
                        compression_codec=task.destination_compression_codec,
                        erasure_codec=task.destination_erasure_codec,
                        default_tmp_dir=self._config.get("default_tmp_dir"))
                else:  # native
                    network_name = "fastbone" if fastbone else "default"
                    network_name = parameters.get("network_name", network_name)
                    copy_yt_to_yt(
                        source_client,
                        destination_client,
                        task.source_table,
                        task.destination_table,
                        network_name=network_name,
                        copy_spec_template=copy_spec,
                        postprocess_spec_template=postprocess_spec,
                        compression_codec=task.destination_compression_codec,
                        erasure_codec=task.destination_erasure_codec)
            elif source_client._type == "yt" and destination_client._type == "yamr":
                logger.info("Running YT -> YAMR remote copy")
                if task.copy_method == "push":
                    copy_yt_to_yamr_push(
                        source_client,
                        destination_client,
                        task.source_table,
                        task.destination_table,
                        fastbone=fastbone,
                        copy_spec_template=copy_spec)
                else:
                    copy_yt_to_yamr_pull(
                        source_client,
                        destination_client,
                        task.source_table,
                        task.destination_table,
                        fastbone=fastbone,
                        force_sort=task.destination_force_sort,
                        default_tmp_dir=self._config.get("default_tmp_dir"))
            elif source_client._type == "yamr" and destination_client._type == "yt":
                logger.info("Running YAMR -> YT remote copy")
                copy_yamr_to_yt_pull(
                    source_client,
                    destination_client,
                    task.source_table,
                    task.destination_table,
                    fastbone=fastbone,
                    compression_codec=task.destination_compression_codec,
                    erasure_codec=task.destination_erasure_codec,
                    force_sort=task.destination_force_sort,
                    copy_spec_template=copy_spec,
                    postprocess_spec_template=postprocess_spec,
                    job_timeout=task.job_timeout)
            elif source_client._type == "yamr" and destination_client._type == "yamr":
                destination_client.remote_copy(
                    source_client.server,
                    task.source_table,
                    task.destination_table,
                    fastbone=fastbone)
            elif source_client._type == "yt" and destination_client._type == "kiwi":
                dc_name = source_client._parameters.get("dc_name")
                if dc_name is not None:
                    copy_spec["scheduling_tag"] = dc_name
                copy_yt_to_kiwi(
                    source_client,
                    destination_client,
                    clusters_configuration.kiwi_transmitter,
                    task.source_table,
                    fastbone=fastbone,
                    kiwi_user=task.kiwi_user,
                    kwworm_options=task.kwworm_options,
                    copy_spec_template=copy_spec,
                    default_tmp_dir=self._config.get("default_tmp_dir"))
            elif source_client._type == "hive" and destination_client._type == "yt":
                copy_hive_to_yt(
                    source_client,
                    destination_client,
                    task.source_table,
                    task.destination_table,
                    copy_spec_template=copy_spec)
            elif (source_client._type == "hdfs" and destination_client._type == "hdfs") \
                    or (source_client._type == "hive" and destination_client._type == "hive") \
                    or (source_client._type == "hbase" and destination_client._type == "hbase"):
                type_to_task_type = {"hdfs": "distcp", "hive": "hivecp", "hbase": "hbasecp"}
                copy_hadoop_to_hadoop_with_airflow(
                    type_to_task_type[source_client._type],
                    clusters_configuration.airflow_client,
                    task.source_table,
                    source_client.airflow_name,
                    task.destination_table,
                    destination_client.airflow_name,
                    task.user,
                    message_queue)
            else:
                raise Exception("Incorrect cluster types: {} source and {} destination".format(
                                source_client._type,
                                destination_client._type))
            if skipped:
                message_queue.put({"type": "skipped"})
            else:
                logger.info("Task completed")
                message_queue.put({"type": "completed"})

            # NB: hack to avoid process died silently.
            time.sleep(0.5)
        except KeyboardInterrupt:
            pass
        except yt.YtError as error:
            try:
                truncate_stderrs_attributes(error, self._config["error_details_length_limit"])
            except Exception:
                logger.exception("Task failed with error:")
                raise
            log_yt_exception(logger, "Task {} failed with error:".format(task.id))
            message_queue.put({
                "type": "error",
                "error": error.simplify()
            })
        except Exception as error:
            logger.exception("Task failed with error:")
            message_queue.put({
                "type": "error",
                "error": {
                    "message": str(error),
                    "code": 1,
                    "attributes": {
                        "details": (traceback_helpers.format_exc() if self._config["enable_detailed_traceback"] else traceback.format_exc())
                    }
                }
            })

    def _get_config(self):
        # NB: Build the config, that actually used by transfer manager.
        config = deepcopy(self._config)
        with self._clusters_config_mutex:
            for name in ["clusters", "availability_graph", "kiwi_transmitter"]:
                config[name] = self._last_successfully_loaded_config[name]
        return config

    def _get_task_description(self, task):
        task_description = task.dict(hide_token=True)
        queue_index = 1
        with self._mutex:
            for id in self._pending_tasks:
                if id == task.id:
                    task_description["queue_index"] = queue_index
                if self._tasks[id].get_queue_id() == task.get_queue_id():
                    queue_index += 1
        return task_description

    def _abort_running_operations(self):
        for task_id, task in self._tasks.iteritems():
            if not task.progress or "operations" not in task.progress:
                continue
            for operation in task.progress["operations"]:
                # NB: To avoid aborting Hadoop tasks.
                # TODO(ignat): support aborting Hadoop task.
                if "id" not in operation:
                    continue
                cluster_name, operation_id = operation["cluster_name"], operation["id"]
                if cluster_name not in self._clusters_configuration.clusters:
                    continue
                if self._clusters_configuration.clusters[cluster_name]._type != "yt":
                    logger.warning("Transfer manager does not support progress for non-YT cluster, "
                                   "but it unexpectedly found for %s cluster." % self._clusters[cluster_name]._type)
                    continue
                try:
                    if not self._yt.exists("//sys/operations/%s" % operation_id) or \
                           self._yt.get_operation_state(operation_id).is_finished():
                        continue
                    self._clusters_configuration.clusters[cluster_name].abort_operation(operation_id)
                    logger.info("Aborted outdated operation {} of task {}".format(operation_id, task_id))
                except yt.YtError:
                    log_yt_exception(logger, "Failed abort operation {} of task {}".format(operation_id, task_id))

    def _create_task(self, params, token, user, dry_run, make_precheck):
        id = generate_uuid()
        logger.info("Adding task %s with id %s", json.dumps(params), id)

        # Move this check to precheck function
        required_parameters = set(["source_cluster", "source_table", "destination_cluster"])
        if not set(params) >= required_parameters:
            raise RequestFailed("All required parameters ({}) must be presented".format(", ".join(required_parameters)))
        if "destination_table" not in params:
            params["destination_table"] = None
        if "backend_tag" not in params:
            params["backend_tag"] = self._config.get("backend_tag")

        try:
            task = Task(id=id, creation_time=now_str(), user=user, token=token, state="pending", **params)
        except TypeError as error:
            raise RequestFailed("Cannot create task", inner_errors=[yt.YtError(error.message)])

        if len(self._pending_tasks) > self._config["pending_tasks_limit"]:
            raise RequestFailed("Too many pending tasks (>{0})".format(len(self._pending_tasks)))

        if self._pending_tasks_per_user[task.user] > self._config["pending_tasks_limit_per_user"]:
            raise RequestFailed("Too many pending tasks per user (>{0})".format(self._pending_tasks_per_user[task.user]))

        clusters_configuration = self._get_clusters_configuration()
        self._set_defaults(task, clusters_configuration)
        if make_precheck:
            try:
                self._precheck(task, clusters_configuration,
                               ignore_timeout=True, yamr_timeout=5.0, custom_logger=TaskIdLogger(task.id))
            except yt.YtError as error:
                raise RequestFailed("Precheck of task {} failed".format(task.id), inner_errors=[error]), None, sys.exc_info()[2]

        if not dry_run:
            self._initialize_pool(task, clusters_configuration)
            self._yt.set(os.path.join(self._tasks_path, task.id), task.dict())
            if task.lease_timeout is not None:
                task._last_ping_time = time.time()
            with self._mutex:
                self._tasks[task.id] = task
                self._append_pending_task(task)
            logger.info("Task %s added", task.id)

        return task.id

    # Public interface
    def run(self, *args, **kwargs):
        logger.info("Starting transfer manager (pid %s)", os.getpid())
        self._daemon.run(*args, **kwargs)


    # Url handlers
    def main(self):
        return "This is YT transfer manager"

    def add(self):
        try:
            params = json.loads_as_bytes(request.data)
        except ValueError as error:
            raise RequestFailed("Cannot parse JSON from body '{}'".format(request.data), inner_errors=[yt.YtError(error.message)])

        token, user = self._get_token_and_user(request.headers)
        dry_run = request.args.get("dry_run", False)

        if isinstance(params, list):
            ids = []
            for task_params in params:
                ids.append(self._create_task(params=task_params, token=token, user=user, dry_run=dry_run, make_precheck=False))
            return ids
        else:
            return self._create_task(params=params, token=token, user=user, dry_run=dry_run, make_precheck=True)

    def abort(self, id):
        if id not in self._tasks:
            raise RequestFailed("Unknown task " + id)

        with self._mutex:
            self._check_permission(id, request.headers, "Aborting")

        waiting_abort = self._abort_task(id)
        if waiting_abort is not None:
            for _ in waiting_abort:
                time.sleep(self._sleep_step)

        return ""

    def restart(self, id):
        if id not in self._tasks:
            raise RequestFailed("Unknown task " + id)

        logger.info("Restarting task %s", id)

        with self._mutex:
            self._check_permission(id, request.headers, "Restarting")
            if self._tasks[id].state not in ["completed", "aborted", "failed"]:
                raise RequestFailed("Cannot restart task in state " + self._tasks[id].state)

            self._tasks[id].state = "pending"
            self._tasks[id].creation_time = now_str()
            self._tasks[id]._last_ping_time = time.time()
            self._tasks[id].finish_time = None
            self._tasks[id].progress = None
            self._tasks[id].error = None
            self._dump_task(id)
            self._pending_tasks.append(id)

        return ""

    def get_task(self, id):
        if id not in self._tasks:
            raise RequestFailed("Unknown task " + id)

        tasks = {}
        with self._mutex:
            pending_tasks = deepcopy(self._pending_tasks)
            tasks[id] = deepcopy(self._tasks[id])

        for task_id in pending_tasks:
            tasks[task_id] = self._tasks[task_id]

        task = tasks[id]
        task_description = task.dict(hide_token=True)
        queue_index = 1
        for task_id in pending_tasks:
            if task_id == id:
                task_description["queue_index"] = queue_index
                break
            if tasks[task_id].get_queue_id() == task.get_queue_id():
                queue_index += 1

        self._renew_lease(id)

        return jsonify(**task_description)

    def ping_task(self, id):
        if id not in self._tasks:
            raise RequestFailed("Unknown task " + id)

        self._renew_lease(id)

        return ""

    def delete_task(self, id):
        if id not in self._tasks:
            raise RequestFailed("Unknown task " + id)

        with self._mutex:
            self._check_permission(id, request.headers, "Deleting")
            if self._tasks[id].state not in ["completed", "aborted", "failed"]:
                raise RequestFailed("Cannot delete running task " + self._tasks[id].state)
            self._yt.remove(os.path.join(self._tasks_path, id), recursive=True)
            del self._tasks[id]

        return ""

    def get_tasks(self):
        with self._mutex:
            # NB: deepcopy is too slow, then we use two-level shallow copy.
            tasks = dict([(id, task.copy()) for id, task in self._tasks.iteritems()])
            # NB: Number of pending tasks is rather small, so we can use deepcopy here.
            pending_tasks = deepcopy(self._pending_tasks)

        user = request.args.get("user")
        fields = request.args.getlist("fields[]")
        if not fields:
            fields = None

        tasks_queue_indexes = {}
        queue_sizes = defaultdict(int)
        for id in pending_tasks:
            queue_id = tasks[id].get_queue_id()
            queue_sizes[queue_id] += 1
            tasks_queue_indexes[id] = queue_sizes[queue_id]

        if user is not None:
            tasks = dict([(id, task) for id, task in tasks.iteritems() if task.user == user])

        tasks_descriptions = []
        for id, task in tasks.iteritems():
            description = task.dict(hide_token=True, fields=fields)
            if id in tasks_queue_indexes:
                description["queue_index"] = tasks_queue_indexes[id]

            try:
                json.dumps(description)
            except UnicodeDecodeError:
                logger.warning("Cannot decode in JSON struct '%s'", repr(description))
                continue

            tasks_descriptions.append(description)

        return Response(json.dumps(tasks_descriptions), mimetype="application/json")

    def config(self):
        return jsonify(self._get_config())

    def ping(self):
        return "OK", 200

    def match(self):
        params = json.loads_as_bytes(request.data)

        source_pattern = params["source_pattern"]
        destination_pattern = params["destination_pattern"]
        source_cluster = params["source_cluster"]

        clusters_configuration = self._get_clusters_configuration()
        if source_cluster not in clusters_configuration.clusters:
            raise yt.YtError("Unknown cluster " + source_cluster)
        client = clusters_configuration.clusters[source_cluster]

        matchings = match_copy_pattern(client, source_pattern, destination_pattern)
        return Response(json.dumps(matchings), mimetype="application/json")

    def profiling(self):
        with self._profiling_mutex:
            profiling = deepcopy(self._profiling)

        for key in profiling["execution_time"]:
            profiling["execution_time"][key] = list(profiling["execution_time"][key])

        return Response(json.dumps(profiling), mimetype="application/json")


def main():
    parser = argparse.ArgumentParser(description="Transfer manager.")
    parser.add_argument("--execute-task", action="store_true", default=False)
    parser.add_argument("--config", required=True)
    args = parser.parse_args()

    if args.execute_task:
        # NB: we do not want see configuration messages for each task.
        logger.LOGGER.setLevel(logging.WARNING)
        app = Application(args.config)
        app.init_logger()
        logger.LOGGER.setLevel(logging.INFO)

        task = Task(**json.loads_as_bytes(sys.stdin.read().strip()))
        message_queue = MessageWriter(sys.stdout)
        app.execute_task(task, message_queue)
    else:
        # We should register sigterm handler only for main transfer manager process.
        signal.signal(signal.SIGTERM, sigterm_handler)

        app = Application(args.config)
        app.start_processes()
        app.run(host=app._config.get("host", "::"), port=app._config["port"], use_reloader=False, threaded=True)
        app.terminate()

if __name__ == "__main__":
    main()
