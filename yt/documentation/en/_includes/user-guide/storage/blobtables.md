# Binary data in tables

This section contains information about how to use tables to store binary data.

## General information { #common }

You sometimes need to save binary data from jobs.
The obvious solution is to write a file from each job to Cypress.
But this method has its drawbacks:

- It creates a high load on the proxy.
- A large number of objects appears in Cypress which makes working with them less efficient.

Therefore, it is preferable to write the binary data in a single table:

1. The data is divided into parts and saved in separate table rows. This table must have a set of key columns that uniquely identify the file: name and path. Besides that, there must be a column responsible for the [BLOB](https://ru.wikipedia.org/wiki/BLOB) number with the data and a column responsible for the data itself. The table must be sorted by a set of key columns and by a column with the BLOB number so that point reads by the key that specifies the file name are possible.
2. For the file, all BLOBs, except for the last one, must have the same size, for example, 4 MB. The columns with data and the BLOB number must be named `data` and `part_index`, respectively.

Tables that meet the described conditions are BLOB tables.

BLOB tables are used within {{product-name}} to store stderr and core files generated by operation jobs.
For example, the table with stderr operations has 3 columns: `job_id`, `part_index`, and `data`, the table is sorted by `job_id` and `part_index`.

## The read_blob_table command { #read_blob_table }

The special `read_blob_table` command is implemented for convenient data reading from BLOB tables.

The command takes the path to the table as input and outputs a binary data stream.
The command also checks that the indexes of read BLOBs start from zero and go without gaps.

The command has the `part_index_column_name` and `data_column_name` parameters that enable you to set the names of columns with the BLOB number and data, respectively. By default, these names are `part_index` and `data`.

Starting the operation and reading job stderr from the table with BLOB:

{% list tabs %}

- Python

   ```bash
   yt.run_map("....; echo 'something' >&2;", "//tmp/input", "//tmp/output", stderr_table="//tmp/stderr_table")
   ```

- CLI

   ```
   yt read-blob-table '<exact={key=["cc26aa85-a694bf6b-3fe0384-963"]}>//tmp/stderr_table'
   ```

{% endlist %}

{% if audience == internal %}

## Skynet { #skynet }


Files stored in a BLOB table can be downloaded via skynet.

{% note info "Note" %}

All chunks in the table must be replicated at the time of download: `@erasure_codec=none`.
You can check the table using the `@erasure_statistics` attribute.

To do this:

1. [Create a table](#create_table), specify the`@enable_skynet_sharing=%true` attribute, and set a special schema.
2. [Write data to the table](#write_table) via write_table or as an operation output.
3. [Create a sharing](#sky_share) via a request to `http://skynet-manager.yt.yandex.net`.
4. [Download the created sharing](#sky_get) `rbtorrent:XXXXX` using the `sky get` command.

{% endnote %}

### Creating a table { #create_table }

Creating a table for a sharing via skynet:

```python
yt.create("table", "//tmp/skynet_share",
    attributes={
        "enable_skynet_sharing": True,
        "optimize_for": "scan",
        "schema": [
            { "group": "meta", "name": "filename", "type": "string", "sort_order": "ascending" },
            { "group": "meta", "name": "part_index", "type": "int64", "sort_order": "ascending" },
            { "group": "meta", "name": "sha1", "type": "string" },
            { "group": "meta", "name": "md5", "type": "string" },
            { "group": "meta", "name": "data_size", "type": "int64" },
            { "group": "data", "name": "data", "type": "string" }
        ],
    })
```

- The `enable_skynet_sharing` attribute tells the system that the table will be downloaded via skynet. This attribute must be set before the data is written to the table.
- The `optimize_for=scan` attribute enables a [column-by-column storage format](../../../user-guide/storage/chunks.md#optimize_for) for the table. It must be specified so that file names and hashes are stored on the disk separately from the file contents.
- To create multiple sharings from one table per each file or group of files, add a column that will identify the file or group of files to the beginning of the column list. In this case, a list of `rbtorrent` for individual files or groups of files will be generated after starting the sharing:

   ```json
   { "group": "meta", "name": "sky_share_id", "sort_order": "ascending", "type": "int64" }
   ```

### Writing data { #write_table}

Writing data to the table:

```python
yt.write_table("//tmp/skynet_share",
    [
        {"filename": "test.bin", "part_index": 0, "data": "x" * 4 * 1024 * 1024},
        {"filename": "test.bin", "part_index": 1, "data": "y" * 4 * 1024 * 1024},
        {"filename": "test.bin", "part_index": 2, "data": "z" * 42},
        {"filename": "test.txt", "part_index": 0, "data": "text"},
    ])
```

- Due to the implementation peculiarities, writing to the table must be performed in one HTTP request. To achieve this, you need to disable `retry` in the client.
- Only the `filename`, `part_index`, and `data` fields need to be written. The `sha1`, `md5`, and `data_size` fields are computed by the table writer itself.
- All parts of the same file must be located in a table sequentially.
- The `data` size must be the same for all pieces of the file, except for the last one. The recommended size is 4 MB.
- You can write to the table from any operation.

### Creating a sharing { #sky_share }

The sharing is created by a separate command. The command returns a row which must then be passed to the `sky get` command to download the file.
Creating a sharing:

{% list tabs %}

- CLI

   ```bash
   yt sky-share --cluster <cluster-name> //tmp/skynet_share
   ```

- Python

   ```python
   rbtorrentid = yt.sky_share("//tmp/skynet_share")
   ```

- HTTP

   ```bash
   curl -X POST -v http://skynet-manager.yt.yandex.net/api/v1/share -H 'X-Yt-Parameters: {cluster=cluster-name; path="//tmp/skynet_share"}'

   curl -X POST -v http://skynet-manager.yt.yandex.net/api/v1/share -H 'X-Yt-Parameters: {cluster=cluster-name; path="//tmp/skynet_share"}'
   rbtorrent:082fac002b6cd8337f10032538e728c99d5a99c8
   ```

   - In the `X-Yt-Parameters` header, you need to pass yson-map with indication of the cluster and the path to the table.
   - Creating a sharing is an asynchronous operation. The first request returns the `202 Accepted` status. You must repeat the request until the `200 OK` status is returned.

{% endlist %}

- You can make a batch request to create sharings by specifying the `key_columns` parameter. The service will read the entire table and create a separate sharing for each unique key value. In this case, the response will not be a single rbtorrent, but JSON with a list of sharings.
- You can use row selection modifiers to create a sharing from a subset of rows in a table. For example, you can write the shard number in each row and at the moment of creation, select the necessary files by requesting `//tmp/skynet_share[1]`.
- Downloading the table via fastbone is enabled by the `enable_fastbone` parameter. If `enable_fastbone = %false`, the table will always be downloaded via backbone. If `enable_fastbone = %true`, the table will be downloaded via fastbone if there are relevant rules in the firewall.
- When the input table is deleted, the sharing will be deleted automatically.
- Any edits that change the `@content_revision` attribute of the input table will result in the automatic deletion of the sharing.
- Input tables in erasure format are not supported.

### Downloading a sharing { #sky_get }

Downloading a sharing:

```bash
sky get rbtorrent:082fac002b6cd8337f10032538e728c99d5a99c8
```
{% endif %}

