commit f491e41933a29d0c234fa475f22180fe987878c0
author: alex-shishkin
date: 2022-10-17T16:46:45+03:00

    user_defined_types

--- taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
+++ taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
@@ -111,7 +111,14 @@ object Cast {
                 toField.nullable)
         }
 
-    case (udt1: UserDefinedType[_], udt2: UserDefinedType[_]) if udt2.acceptsType(udt1) => true
+    case (udt1: UserDefinedType[_], udt2: UserDefinedType[_])if udt2.userClass == udt1.userClass =>
+      true
+
+    case (udt: UserDefinedType[_], toType) if udt.sqlType == toType =>
+      true
+
+    case (fromType, udt: UserDefinedType[_]) if udt.sqlType == fromType =>
+      true
 
     case _ => false
   }
@@ -876,6 +883,10 @@ abstract class CastBase extends UnaryExpression with TimeZoneAwareExpression wit
     })
   }
 
+  private[this] def udtSqlTypeEquals(dt: DataType, udt: DataType): Boolean = {
+    udt.isInstanceOf[UserDefinedType[_]] && udt.asInstanceOf[UserDefinedType[_]].sqlType == dt
+  }
+
   protected[this] def cast(from: DataType, to: DataType): Any => Any = {
     // If the cast does not change the structure, then we don't really need to cast anything.
     // We can return what the children return. Same thing should happen in the codegen path.
@@ -890,6 +901,7 @@ abstract class CastBase extends UnaryExpression with TimeZoneAwareExpression wit
     } else {
       to match {
         case dt if dt == from => identity[Any]
+        case dt if udtSqlTypeEquals(dt, from) => identity[Any]
         case StringType => castToString(from)
         case BinaryType => castToBinary(from)
         case DateType => castToDate(from)
@@ -910,6 +922,7 @@ abstract class CastBase extends UnaryExpression with TimeZoneAwareExpression wit
           castArray(from.asInstanceOf[ArrayType].elementType, array.elementType)
         case map: MapType => castMap(from.asInstanceOf[MapType], map)
         case struct: StructType => castStruct(from.asInstanceOf[StructType], struct)
+        case udt: UserDefinedType[_] if udtSqlTypeEquals(from, udt) => identity[Any]
         case udt: UserDefinedType[_] if udt.acceptsType(from) =>
           identity[Any]
         case _: UserDefinedType[_] =>
@@ -951,6 +964,9 @@ abstract class CastBase extends UnaryExpression with TimeZoneAwareExpression wit
 
     case _ if from == NullType => (c, evPrim, evNull) => code"$evNull = true;"
     case _ if to == from => (c, evPrim, evNull) => code"$evPrim = $c;"
+    case _ if from.isInstanceOf[UserDefinedType[_]] &&
+      from.asInstanceOf[UserDefinedType[_]].sqlType == to =>
+      (c, evPrim, evNull) => code"$evPrim = $c;"
     case StringType => castToStringCode(from, ctx)
     case BinaryType => castToBinaryCode(from)
     case DateType => castToDateCode(from, ctx)
@@ -972,8 +988,18 @@ abstract class CastBase extends UnaryExpression with TimeZoneAwareExpression wit
       castArrayCode(from.asInstanceOf[ArrayType].elementType, array.elementType, ctx)
     case map: MapType => castMapCode(from.asInstanceOf[MapType], map, ctx)
     case struct: StructType => castStructCode(from.asInstanceOf[StructType], struct, ctx)
-    case udt: UserDefinedType[_] if udt.acceptsType(from) =>
-      (c, evPrim, evNull) => code"$evPrim = $c;"
+    case udt: UserDefinedType[_]
+      if udt.sqlType == from || udt.userClass == from.asInstanceOf[UserDefinedType[_]].userClass =>
+      to match {
+        case udt: UserDefinedType[_] with ValidatedCastType =>
+          val udtRef = JavaCode.global(ctx.addReferenceObj("udt", udt), udt.sqlType)
+          (c, evPrim, evNull) =>
+            code"""
+                  |$udtRef.validate($c);
+                  |$evPrim = $c;
+                  """.stripMargin
+        case _ => (c, evPrim, evNull) => code"$evPrim = $c;"
+      }
     case _: UserDefinedType[_] =>
       throw QueryExecutionErrors.cannotCastError(from, to)
   }
--- taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SpecificInternalRow.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
+++ taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SpecificInternalRow.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
@@ -203,6 +203,7 @@ final class SpecificInternalRow(val values: Array[MutableValue]) extends BaseGen
     case BooleanType => new MutableBoolean
     case ByteType => new MutableByte
     case ShortType => new MutableShort
+    case udt: UserDefinedType[_] => dataTypeToMutableValue(udt.sqlType)
     case _ => new MutableAny
   }
 
--- taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
+++ taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
@@ -616,6 +616,7 @@ class CodegenContext extends Logging {
    * Generates code for equal expression in Java.
    */
   def genEqual(dataType: DataType, c1: String, c2: String): String = dataType match {
+    case audt: AggregatingUserDefinedType[_] => genComp(audt, c1, c2) + " == 0"
     case BinaryType => s"java.util.Arrays.equals($c1, $c2)"
     case FloatType =>
       s"((java.lang.Float.isNaN($c1) && java.lang.Float.isNaN($c2)) || $c1 == $c2)"
@@ -640,6 +641,7 @@ class CodegenContext extends Logging {
    * @param c2 name of the variable of expression 2's output
    */
   def genComp(dataType: DataType, c1: String, c2: String): String = dataType match {
+    case audt: AggregatingUserDefinedType[_] => audt.compareGen(c1, c2)
     // java boolean doesn't support > or < operator
     case BooleanType => s"($c1 == $c2 ? 0 : ($c1 ? 1 : -1))"
     case DoubleType =>
--- taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
+++ taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
@@ -69,6 +69,10 @@ class InterpretedOrdering(ordering: Seq[SortOrder]) extends BaseOrdering {
             s.interpretedOrdering.asInstanceOf[Ordering[Any]].compare(left, right)
           case s: StructType if order.direction == Descending =>
             - s.interpretedOrdering.asInstanceOf[Ordering[Any]].compare(left, right)
+          case a: AggregatingUserDefinedType[_] if order.direction == Ascending =>
+            a.ordering.compare(left, right)
+          case a: AggregatingUserDefinedType[_] if order.direction == Descending =>
+            a.ordering.reverse.compare(left, right)
           case other =>
             throw QueryExecutionErrors.orderedOperationUnsupportedByDataTypeError(other)
         }
--- taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/types/UserDefinedType.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
+++ taxi/dmp/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/types/UserDefinedType.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
@@ -97,6 +97,15 @@ abstract class UserDefinedType[UserType >: Null] extends DataType with Serializa
   override def catalogString: String = sqlType.simpleString
 }
 
+trait AggregatingUserDefinedType[UserType >: Null] {
+  self: UserDefinedType[UserType] =>
+  def hashGen(name: String): String
+
+  def compareGen(first: String, second: String): String
+
+  val ordering: Ordering[Any]
+}
+
 private[spark] object UserDefinedType {
   /**
    * Get the sqlType of a (potential) [[UserDefinedType]].
@@ -107,6 +116,10 @@ private[spark] object UserDefinedType {
   }
 }
 
+trait ValidatedCastType {
+  def validate(datum: Any): Unit
+}
+
 /**
  * The user defined type in Python.
  *
--- taxi/dmp/spark/spark/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala	(e06d1e926f1cd797552cf0f9b72e9642599d3dd4)
+++ taxi/dmp/spark/spark/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala	(f491e41933a29d0c234fa475f22180fe987878c0)
@@ -155,6 +155,7 @@ abstract class HashMapGenerator(
          |}
        """.stripMargin
     }
+    def hashUDT(h: String): String = s"int $result = $h;"
 
     dataType match {
       case BooleanType => hashInt(s"$input ? 1 : 0")
@@ -174,6 +175,8 @@ abstract class HashMapGenerator(
           """
         }
       case StringType => hashBytes(s"$input.getBytes()")
+      case audt: AggregatingUserDefinedType[_] => hashUDT(audt.hashGen(input))
+      case udt: UserDefinedType[_] => hashUDT(s"$input.hashCode()")
     }
   }
 }
