commit a3903af4981cb29aced219e5df6a6fbb640f49c0
author: alex-shishkin
date: 2022-10-17T16:55:36+03:00

    python_features

--- taxi/dmp/spark/spark/python/pyspark/context.py	(ab5923972943a6111392d00eecabd1110e96e682)
+++ taxi/dmp/spark/spark/python/pyspark/context.py	(a3903af4981cb29aced219e5df6a6fbb640f49c0)
@@ -125,7 +125,7 @@ class SparkContext(object):
     _lock = RLock()
     _python_includes = None  # zip and egg files that need to be added to PYTHONPATH
 
-    PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')
+    PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar', '.whl')
 
     def __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None,
                  environment=None, batchSize=0, serializer=PickleSerializer(), conf=None,
--- taxi/dmp/spark/spark/python/pyspark/sql/dataframe.py	(ab5923972943a6111392d00eecabd1110e96e682)
+++ taxi/dmp/spark/spark/python/pyspark/sql/dataframe.py	(a3903af4981cb29aced219e5df6a6fbb640f49c0)
@@ -2477,6 +2477,16 @@ class DataFrame(PandasMapOpsMixin, PandasConversionMixin):
             raise TypeError("col should be Column")
         return DataFrame(self._jdf.withColumn(colName, col._jc), self.sql_ctx)
 
+    def withYsonColumn(self, colName, col):
+        java_column = _to_java_column(col)
+        return DataFrame(
+            self._sc._jvm.ru.yandex.spark.yt.PythonUtils.withYsonColumn(self._jdf, colName, java_column),
+            self.sql_ctx
+        )
+
+    def transform(self, func):
+        return func(self)
+
     def withColumnRenamed(self, existing, new):
         """Returns a new :class:`DataFrame` by renaming an existing column.
         This is a no-op if schema doesn't contain the given column name.
--- taxi/dmp/spark/spark/python/pyspark/sql/readwriter.py	(ab5923972943a6111392d00eecabd1110e96e682)
+++ taxi/dmp/spark/spark/python/pyspark/sql/readwriter.py	(a3903af4981cb29aced219e5df6a6fbb640f49c0)
@@ -20,7 +20,7 @@ from py4j.java_gateway import JavaClass
 
 from pyspark import RDD, since
 from pyspark.sql.column import _to_seq, _to_java_column
-from pyspark.sql.types import StructType
+from pyspark.sql.types import *
 from pyspark.sql import utils
 from pyspark.sql.utils import to_str
 
@@ -300,6 +300,30 @@ class DataFrameReader(OptionUtils):
 
         return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
 
+    def yt(self, *paths):
+        def fix_path(path):
+            return path[1:] if path.startswith("//") else path
+        return self.format("yt").load(path=[fix_path(path) for path in paths])
+
+    def _dict_to_struct(self, dict_type):
+        return StructType([StructField(name, data_type) for (name, data_type) in dict_type.items()])
+
+    def schema_hint(self, fields):
+        from pyspark.sql import SparkSession
+        spark = SparkSession.builder.getOrCreate()
+
+        struct_fields = []
+        for name, data_type in fields.items():
+            if isinstance(data_type, dict):
+                data_type = self._dict_to_struct(data_type)
+            field = StructField(name, data_type)
+            struct_fields.append(field)
+        schema = StructType(struct_fields)
+
+        jschema = spark._jsparkSession.parseDataType(schema.json())
+        self._jreader = spark._jvm.ru.yandex.spark.yt.PythonUtils.schemaHint(self._jreader, jschema)
+        return self
+
     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,
              recursiveFileLookup=None, modifiedBefore=None,
              modifiedAfter=None):
@@ -845,6 +869,20 @@ class DataFrameWriter(OptionUtils):
             lineSep=lineSep, encoding=encoding, ignoreNullFields=ignoreNullFields)
         self._jwrite.json(path)
 
+    def yt(self, path, mode=None):
+        def fix_path(path):
+            return path[1:] if path.startswith("//") else path
+        self.mode(mode)
+        self.format("yt").save(fix_path(path))
+
+    def sorted_by(self, *cols, **kwargs):
+        unique_keys = kwargs.get("unique_keys") or False
+        import json
+        return self.option("sort_columns", json.dumps(cols)).option("unique_keys", str(unique_keys))
+
+    def optimize_for(self, optimize_mode):
+        return self.option("optimize_for", optimize_mode)
+
     def parquet(self, path, mode=None, partitionBy=None, compression=None):
         """Saves the content of the :class:`DataFrame` in Parquet format at the specified path.
 
